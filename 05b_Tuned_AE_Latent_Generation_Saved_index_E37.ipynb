{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09327dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import glob\n",
    "import sklearn \n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, optimizers\n",
    "from tensorflow.keras.layers import Input, Activation, Dense, LeakyReLU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras_tuner import BayesianOptimization, HyperParameters\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e0bf33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data import\n",
    "wt_filtered = ['wt_filtered_lcc_3_50.lccdata', 'wt_filtered_lcc_12_50.lccdata', 'wt_filtered_lcc_20_50.lccdata']\n",
    "\n",
    "# filtered wt LCC data import\n",
    "wt_f_var_names = ['wt_3f', 'wt_12f', 'wt_20f']\n",
    "\n",
    "for var, file in zip(wt_f_var_names, wt_filtered):\n",
    "    globals()[var] = pd.read_csv(file, sep='\\t').drop(columns='Unnamed: 0')\n",
    "\n",
    "# filtered mutant LCC data import\n",
    "D132H_filtered = ['D132H_filtered_lcc_3_50.lccdata', 'D132H_filtered_lcc_12_50.lccdata', 'D132H_filtered_lcc_20_50.lccdata']\n",
    "D132H_f_var_names = ['D132H_3f', 'D132H_12f', 'D132H_20f']\n",
    "\n",
    "for var, file in zip(D132H_f_var_names, D132H_filtered):\n",
    "    globals()[var] = pd.read_csv(file, sep='\\t').drop(columns='Unnamed: 0')\n",
    "    \n",
    "# Visualization of dataset\n",
    "print('WT for window size = 3')\n",
    "display(wt_3f)\n",
    "print('WT for window size = 12')\n",
    "display(wt_12f)\n",
    "print('WT for window size = 20')\n",
    "display(wt_20f)\n",
    "\n",
    "print('\\n')\n",
    "print('---------------------------------')\n",
    "print('D132H for window size = 3')\n",
    "display(D132H_3f)\n",
    "print('D132H for window size = 12')\n",
    "display(D132H_12f)\n",
    "print('D132H for window size = 20')\n",
    "display(D132H_20f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc552490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concateneate wt and mutant dataframes and rename columns\n",
    "\n",
    "wt_f = pd.concat([wt_3f, wt_12f, wt_20f], axis = 1)\n",
    "    \n",
    "D132H_f = pd.concat([D132H_3f, D132H_12f, D132H_20f], axis = 1)\n",
    "D132H_f.index = range(40000, 40000 + len(D132H_f)) # Modifying index of D132H so that there are no duplicate indices\n",
    "\n",
    "colnames = [*range(0,12)]\n",
    "colnames\n",
    "wt_f.columns = colnames\n",
    "D132H_f.columns = colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(wt, mutant):\n",
    "    \n",
    "    wt_label = np.zeros(len(wt))  # Set wt labels to 0\n",
    "    mutant_label = np.ones(len(mutant))  # Set mutant labels to 1\n",
    "\n",
    "    # Create label dataframes with indices\n",
    "    wt_label_df = pd.DataFrame({'class': wt_label})\n",
    "    mutant_label_df = pd.DataFrame({'class': mutant_label}, index=range(40000, 40000 + len(mutant)))\n",
    "\n",
    "    # Concatenate data frames and label dataframes\n",
    "    X_train_full = pd.concat([wt, mutant])\n",
    "    y_train_full_df = pd.concat([wt_label_df, mutant_label_df])\n",
    "\n",
    "    # Normalize training data\n",
    "    X_train_full = X_train_full.div(100)  # Adjust as necessary\n",
    "\n",
    "    # Separate training and validation sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full_df, stratify=y_train_full_df['class'], test_size=0.2)\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(X_valid.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_valid.shape)\n",
    "\n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3072ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f, X_valid_f, y_train_f, y_valid_f = preprocessing(wt_f, D132H_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f91c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get autoencoder model\n",
    "def get_ae(train_data, LeReLU_alpha=0.01):\n",
    "    \n",
    "    #Input layer\n",
    "    input_layer = Input(shape=(train_data.shape[1]), name='ae_input')\n",
    "    \n",
    "    encoder = Dense(336, activation=LeakyReLU(alpha=LeReLU_alpha), name='e1')(input_layer)\n",
    "    encoder = Dense(208, activation=LeakyReLU(alpha=LeReLU_alpha), name='e2')(encoder)\n",
    "    encoder = Dense(240, activation=LeakyReLU(alpha=LeReLU_alpha), name='e3')(encoder)\n",
    "\n",
    "    encoded = Dense(2, activation=LeakyReLU(alpha=LeReLU_alpha), name='ae_latent')(encoder)\n",
    "    \n",
    "    decoder = Dense(240, activation=LeakyReLU(alpha=LeReLU_alpha), name='d1')(encoded)\n",
    "    decoder = Dense(208, activation=LeakyReLU(alpha=LeReLU_alpha), name='d2')(decoder)\n",
    "    decoder = Dense(336, activation=LeakyReLU(alpha=LeReLU_alpha), name='d3')(decoder)\n",
    "\n",
    "    output_layer = Dense(train_data.shape[1], activation=LeakyReLU(alpha=LeReLU_alpha), name='ae_output')(decoder)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f15476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ae for filtered data\n",
    "autoencoder = get_ae(X_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0364035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of ae model\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "autoencoder.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam(learning_rate = 0.00001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1088d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"AET_CF_E37_Trial_1\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "name = \"1_LSP_AET_37_CF_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac9dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f.to_csv(f'{folder_name}/X_train_f.csv')\n",
    "y_train_f.to_csv(f'{folder_name}/y_train_f.csv')\n",
    "\n",
    "X_valid_f.to_csv(f'{folder_name}/X_valid_f.csv')\n",
    "y_valid_f.to_csv(f'{folder_name}/y_valid_f.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe077b70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for counts in tqdm(range(38)):\n",
    "    output_path = os.path.join(folder_name, name + f\"Predictions{counts}.txt\")\n",
    "    with open(output_path, \"w\") as file:\n",
    "        history = autoencoder.fit(X_train_f, X_train_f, batch_size=256, epochs=1000,\n",
    "                                  validation_data=(X_valid_f, X_valid_f), verbose=1)\n",
    "\n",
    "        training_history = pd.DataFrame(history.history)\n",
    "        plt.plot(training_history)\n",
    "        file_name_0 = os.path.join(folder_name, name + \"Training_History\" + str(counts))\n",
    "        training_history.to_pickle(file_name_0)\n",
    "        file_name_1 = os.path.join(folder_name, name + \"Training_History\" + str(counts) + \".png\")\n",
    "        plt.savefig(file_name_1, dpi=300)\n",
    "        plt.clf()\n",
    "\n",
    "        dr_model = tf.keras.models.Model(inputs=autoencoder.get_layer('ae_input').input,\n",
    "                                         outputs=autoencoder.get_layer('ae_latent').output)\n",
    "        dr_model.summary(print_fn=lambda x: file.write(x + '\\n'))\n",
    "\n",
    "        batch_size = 32\n",
    "        x = []\n",
    "        y = []\n",
    "        z = []\n",
    "        indices = []\n",
    "\n",
    "        for batch_start in range(0, len(X_valid_f), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(X_valid_f))\n",
    "            X_batch = np.array(X_valid_f.iloc[batch_start:batch_end])\n",
    "            y_batch = y_valid_f.iloc[batch_start:batch_end]\n",
    "\n",
    "            op_batch = dr_model.predict(X_batch, verbose=0)\n",
    "\n",
    "            for i, op in enumerate(op_batch):\n",
    "                z.append(y_batch.iloc[i]['class'])  # Access class label correctly\n",
    "                x.append(op[0])\n",
    "                y.append(op[1])\n",
    "                indices.append(y_batch.iloc[i].name)  # Using .name to get the index of the row\n",
    "                file.write(f\"Prediction {batch_start + i}: {op}\\n\")\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        df['x'] = x\n",
    "        df['y'] = y\n",
    "        df['z'] = [\"trajectory-\" + str(k) for k in z]\n",
    "        df['index'] = indices\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        fig = sns.scatterplot(x='x', y='y', hue='z', data=df, s=10)\n",
    "        file_name_2 = os.path.join(folder_name, name + 'Predictions' + str(counts) + \".png\")\n",
    "        fig.figure.savefig(file_name_2, dpi=300)\n",
    "        plt.clf()\n",
    "\n",
    "        file_name_3 = os.path.join(folder_name, name + 'Predictions' + str(counts))\n",
    "        df.to_pickle(file_name_3)\n",
    "\n",
    "        model_folder = os.path.join(folder_name, 'models')\n",
    "        if not os.path.exists(model_folder):\n",
    "            os.makedirs(model_folder)\n",
    "        file_name = os.path.join(model_folder, 'saved_model_' + name + str(counts))\n",
    "        autoencoder.save(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
