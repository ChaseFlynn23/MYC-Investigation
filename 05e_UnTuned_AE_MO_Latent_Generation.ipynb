{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09327dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import glob\n",
    "import sklearn \n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, optimizers\n",
    "from tensorflow.keras.layers import Input, Activation, Dense, LeakyReLU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras_tuner import BayesianOptimization, HyperParameters"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0920289",
   "metadata": {},
   "source": [
    "import csv\n",
    "\n",
    "# Function to delete specific columns and save to a new file\n",
    "def delete_columns(input_file_path, output_file_path, columns_to_delete):\n",
    "    modified_lines = []\n",
    "\n",
    "    with open(input_file_path, 'r', newline='') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            # Delete specified columns (adjusting for zero-based indexing)\n",
    "            row = [item for idx, item in enumerate(row) if idx not in columns_to_delete]\n",
    "            modified_lines.append(row)\n",
    "\n",
    "    # Save the modified content to the new file\n",
    "    with open(output_file_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter='\\t')\n",
    "        writer.writerows(modified_lines)\n",
    "\n",
    "# Input and output file paths\n",
    "input_file_path = '/home/chaseflynn/MYC-Investigation/Final-Year-Project-main/wt_filtered_lcc_12_50.lccdata'\n",
    "output_file_path = '/home/chaseflynn/MYC-Investigation/Final-Year-Project-main/wt_filtered_2_lcc_12_50.lccdata'\n",
    "\n",
    "# Columns to delete (adjusting for zero-based indexing)\n",
    "columns_to_delete = [1, 3]\n",
    "\n",
    "# Perform the deletion\n",
    "delete_columns(input_file_path, output_file_path, columns_to_delete)\n",
    "\n",
    "# Print a confirmation message\n",
    "print(\"Columns 6 and 17 have been deleted and the file has been saved as 'wt_filtered_2_lcc_12_50.lccdata'\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a0d3b63",
   "metadata": {},
   "source": [
    "import csv\n",
    "\n",
    "# Function to delete specific columns and save to a new file\n",
    "def delete_columns(input_file_path, output_file_path, columns_to_delete):\n",
    "    modified_lines = []\n",
    "\n",
    "    with open(input_file_path, 'r', newline='') as file:\n",
    "        reader = csv.reader(file, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            # Delete specified columns (adjusting for zero-based indexing)\n",
    "            row = [item for idx, item in enumerate(row) if idx not in columns_to_delete]\n",
    "            modified_lines.append(row)\n",
    "\n",
    "    # Save the modified content to the new file\n",
    "    with open(output_file_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter='\\t')\n",
    "        writer.writerows(modified_lines)\n",
    "\n",
    "# Input and output file paths\n",
    "input_file_path = '/home/chaseflynn/MYC-Investigation/Final-Year-Project-main/D132H_filtered_lcc_12_50.lccdata'\n",
    "output_file_path = '/home/chaseflynn/MYC-Investigation/Final-Year-Project-main/D132H_filtered_2_lcc_12_50.lccdata'\n",
    "\n",
    "# Columns to delete (adjusting for zero-based indexing)\n",
    "columns_to_delete = [1, 3]\n",
    "\n",
    "# Perform the deletion\n",
    "delete_columns(input_file_path, output_file_path, columns_to_delete)\n",
    "\n",
    "# Print a confirmation message\n",
    "print(\"Columns 6 and 17 have been deleted and the file has been saved as 'D132H_filtered_2_lcc_12_50.lccdata'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e0bf33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data import\n",
    "wt_filtered = ['wt_filtered_lcc_3_50.lccdata', 'wt_filtered_2_lcc_12_50.lccdata', 'wt_filtered_lcc_20_50.lccdata']\n",
    "\n",
    "# filtered wt LCC data import\n",
    "wt_f_var_names = ['wt_3f', 'wt_12f', 'wt_20f']\n",
    "\n",
    "for var, file in zip(wt_f_var_names, wt_filtered):\n",
    "    globals()[var] = pd.read_csv(file, sep='\\t').drop(columns='Unnamed: 0')\n",
    "\n",
    "# filtered mutant LCC data import\n",
    "D132H_filtered = ['D132H_filtered_lcc_3_50.lccdata', 'D132H_filtered_2_lcc_12_50.lccdata', 'D132H_filtered_lcc_20_50.lccdata']\n",
    "D132H_f_var_names = ['D132H_3f', 'D132H_12f', 'D132H_20f']\n",
    "\n",
    "for var, file in zip(D132H_f_var_names, D132H_filtered):\n",
    "    globals()[var] = pd.read_csv(file, sep='\\t').drop(columns='Unnamed: 0')\n",
    "    \n",
    "# Visualization of dataset\n",
    "print('WT for window size = 3')\n",
    "display(wt_3f)\n",
    "print('WT for window size = 12')\n",
    "display(wt_12f)\n",
    "print('WT for window size = 20')\n",
    "display(wt_20f)\n",
    "\n",
    "print('\\n')\n",
    "print('---------------------------------')\n",
    "print('D132H for window size = 3')\n",
    "display(D132H_3f)\n",
    "print('D132H for window size = 12')\n",
    "display(D132H_12f)\n",
    "print('D132H for window size = 20')\n",
    "display(D132H_20f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc552490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concateneate wt and mutant dataframes and rename columns\n",
    "\n",
    "wt_f = pd.concat([wt_12f, wt_20f, wt_3f], axis = 1)\n",
    "    \n",
    "D132H_f = pd.concat([D132H_12f, D132H_20f, D132H_3f], axis = 1)\n",
    "\n",
    "colnames = [*range(0,10)]\n",
    "colnames\n",
    "wt_f.columns = colnames\n",
    "D132H_f.columns = colnames\n",
    "\n",
    "# Visualization of dataset\n",
    "print('Filtered wt data')\n",
    "display(wt_f)\n",
    "\n",
    "print('\\n')\n",
    "print('---------------------------------')\n",
    "print('Filtered D132H data')\n",
    "display(D132H_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9faae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre processing\n",
    "\n",
    "def preprocessing(wt, mutant):\n",
    "    \n",
    "    wt_label = np.zeros(len(wt)) # Set wt labels to 0\n",
    "    \n",
    "    mutant_label = np.ones(len(mutant))\n",
    "    \n",
    "    # Concatenate data frames and label arrays\n",
    "\n",
    "    X_train_full = pd.concat([wt.reset_index(), mutant.reset_index()])\n",
    "    y_train_full = np.concatenate((wt_label, mutant_label))\n",
    "    \n",
    "    #Drop index column and normalise training data\n",
    "    X_train_full = X_train_full.drop(columns = 'index')\n",
    "    \n",
    "    X_train_full= X_train_full.div(100) ## When changed from 100 to 56.1035, errors generated.\n",
    "    \n",
    "    # Separate training and validation sets and print relevant shapes\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, stratify=y_train_full, test_size=0.2)\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(X_valid.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_valid.shape)\n",
    "    \n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3072ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f, X_valid_f, y_train_f, y_valid_f = preprocessing(wt_f, D132H_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f91c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get autoencoder model\n",
    "def get_ae(train_data, LeReLU_alpha=0.01):\n",
    "    \n",
    "    #Input layer\n",
    "    input_layer = Input(shape=(train_data.shape[1]), name='ae_input')\n",
    "    \n",
    "    encoder = Dense(256, activation=LeakyReLU(alpha=LeReLU_alpha), name='e1')(input_layer)\n",
    "    encoder = Dense(64, activation=LeakyReLU(alpha=LeReLU_alpha), name='e2')(encoder)\n",
    "\n",
    "    encoded = Dense(2, activation=LeakyReLU(alpha=LeReLU_alpha), name='ae_latent')(encoder)\n",
    "    \n",
    "    decoder = Dense(64, activation=LeakyReLU(alpha=LeReLU_alpha), name='d1')(encoded)\n",
    "    decoder = Dense(256, activation=LeakyReLU(alpha=LeReLU_alpha), name='d2')(decoder)\n",
    "\n",
    "    output_layer = Dense(train_data.shape[1], activation=LeakyReLU(alpha=LeReLU_alpha), name='ae_output')(decoder)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f15476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ae for filtered data\n",
    "autoencoder = get_ae(X_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0364035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary of ae model\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "autoencoder.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam(learning_rate = 0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f764a59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Template to copy and paste\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "# New folder name\n",
    "folder_name = \"AE_UT_MO_Trial_#\"\n",
    "\n",
    "# Check if the folder exists. If not, create it.\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "for counts in tqdm(range(59)):  # this determines the number of epoch sets\n",
    "    # Open a file for this epoch set to write outputs\n",
    "    name = \"#_LSP_MO_UT\"\n",
    "    output_path = os.path.join(folder_name, f\"#_LSP_MO_UT_Predictions_{counts}.txt\")  # Update the path\n",
    "    with open(output_path, \"w\") as file:\n",
    "        # Autoencoder training\n",
    "        history = autoencoder.fit(X_train_f, X_train_f, batch_size=256,\n",
    "                                  epochs=1000, validation_data=(X_valid_f, X_valid_f), verbose=1)\n",
    "        \n",
    "        # Convert history object to dataframe and plot rates\n",
    "        training_history = pd.DataFrame(history.history)\n",
    "        plt.plot(training_history)\n",
    "        file_name_0 = os.path.join(folder_name, name + \"_Training_History\" + str(counts))\n",
    "        training_history.to_pickle(file_name_0)\n",
    "        file_name_1 = os.path.join(folder_name, name + str(counts) + \"_#1.png\")\n",
    "        plt.savefig(file_name_1, dpi=300)\n",
    "        plt.clf()\n",
    "\n",
    "        # Read in latent layer\n",
    "        dr_model = tf.keras.models.Model(inputs=autoencoder.get_layer('ae_input').input,\n",
    "                                         outputs=autoencoder.get_layer('ae_latent').output)\n",
    "        # Write the model summary to the file\n",
    "        dr_model.summary(print_fn=lambda x: file.write(x + '\\n'))\n",
    "\n",
    "        # Define batch size for processing validation data\n",
    "        batch_size = 32\n",
    "\n",
    "        # Initialize lists to hold prediction results\n",
    "        x = []\n",
    "        y = []\n",
    "        z = []\n",
    "\n",
    "        # Process validation data in batches\n",
    "        for batch_start in range(0, len(X_valid_f), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(X_valid_f))\n",
    "            X_batch = np.array(X_valid_f.iloc[batch_start:batch_end])\n",
    "            y_batch = y_valid_f[batch_start:batch_end]\n",
    "\n",
    "            # Make predictions for this batch\n",
    "            op_batch = dr_model.predict(X_batch, verbose=0)\n",
    "\n",
    "            # Process and store the results\n",
    "            for i, op in enumerate(op_batch):\n",
    "                z.append(y_batch[i])\n",
    "                x.append(op[0])\n",
    "                y.append(op[1])\n",
    "                file.write(f\"Prediction {batch_start + i}: {op}\\n\")\n",
    "\n",
    "        # Construct and save the data frame\n",
    "        df = pd.DataFrame()\n",
    "        df['x'] = x\n",
    "        df['y'] = y\n",
    "        df['z'] = [\"trajectory-\" + str(k) for k in z]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        fig = sns.scatterplot(x='x', y='y', hue='z', data=df, s=10)\n",
    "        file_name_2 = os.path.join(folder_name, name + str(counts) + \"_#2.png\")\n",
    "        fig.figure.savefig(file_name_2, dpi=300)\n",
    "        plt.clf()\n",
    "\n",
    "        file_name_3 = os.path.join(folder_name, '#_LSP_MO_UT_Predictions' + str(counts))\n",
    "        df.to_pickle(file_name_3)\n",
    "\n",
    "        # Save the model in a subfolder within Trial folder\n",
    "        model_folder = os.path.join(folder_name, 'models')\n",
    "        if not os.path.exists(model_folder):\n",
    "            os.makedirs(model_folder)\n",
    "        file_name = os.path.join(model_folder, 'saved_model_#_LSP_MO_UT' + str(counts))\n",
    "        autoencoder.save(file_name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4c8543f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Template to copy and paste\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "# New folder name\n",
    "folder_name = \"AE_UT_MO_Trial_#\"\n",
    "\n",
    "# Check if the folder exists. If not, create it.\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "for counts in tqdm(range(59)):  # this determines the number of epoch sets\n",
    "    # Open a file for this epoch set to write outputs\n",
    "    name = \"#_LSP_MO_UT\"\n",
    "    output_path = os.path.join(folder_name, f\"#_LSP_MO_UT_Predictions_{counts}.txt\")  # Update the path\n",
    "    with open(output_path, \"w\") as file:\n",
    "        # Autoencoder training\n",
    "        history = autoencoder.fit(X_train_f, X_train_f, batch_size=256,\n",
    "                                  epochs=1000, validation_data=(X_valid_f, X_valid_f), verbose=1)\n",
    "        \n",
    "        # Convert history object to dataframe and plot rates\n",
    "        training_history = pd.DataFrame(history.history)\n",
    "        plt.plot(training_history)\n",
    "        file_name_0 = os.path.join(folder_name, name + \"_Training_History\" + str(counts))\n",
    "        training_history.to_pickle(file_name_0)\n",
    "        file_name_1 = os.path.join(folder_name, name + str(counts) + \"_#1.png\")\n",
    "        plt.savefig(file_name_1, dpi=300)\n",
    "        plt.clf()\n",
    "\n",
    "        # Read in latent layer\n",
    "        dr_model = tf.keras.models.Model(inputs=autoencoder.get_layer('ae_input').input,\n",
    "                                         outputs=autoencoder.get_layer('ae_latent').output)\n",
    "        # Write the model summary to the file\n",
    "        dr_model.summary(print_fn=lambda x: file.write(x + '\\n'))\n",
    "\n",
    "        # Define batch size for processing validation data\n",
    "        batch_size = 32\n",
    "\n",
    "        # Initialize lists to hold prediction results\n",
    "        x = []\n",
    "        y = []\n",
    "        z = []\n",
    "\n",
    "        # Process validation data in batches\n",
    "        for batch_start in range(0, len(X_valid_f), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(X_valid_f))\n",
    "            X_batch = np.array(X_valid_f.iloc[batch_start:batch_end])\n",
    "            y_batch = y_valid_f[batch_start:batch_end]\n",
    "\n",
    "            # Make predictions for this batch\n",
    "            op_batch = dr_model.predict(X_batch, verbose=0)\n",
    "\n",
    "            # Process and store the results\n",
    "            for i, op in enumerate(op_batch):\n",
    "                z.append(y_batch[i])\n",
    "                x.append(op[0])\n",
    "                y.append(op[1])\n",
    "                file.write(f\"Prediction {batch_start + i}: {op}\\n\")\n",
    "\n",
    "        # Construct and save the data frame\n",
    "        df = pd.DataFrame()\n",
    "        df['x'] = x\n",
    "        df['y'] = y\n",
    "        df['z'] = [\"trajectory-\" + str(k) for k in z]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        fig = sns.scatterplot(x='x', y='y', hue='z', data=df, s=10)\n",
    "        file_name_2 = os.path.join(folder_name, name + str(counts) + \"_#2.png\")\n",
    "        fig.figure.savefig(file_name_2, dpi=300)\n",
    "        plt.clf()\n",
    "\n",
    "        file_name_3 = os.path.join(folder_name, '#_LSP_MO_UT_Predictions' + str(counts))\n",
    "        df.to_pickle(file_name_3)\n",
    "\n",
    "        # Save the model in a subfolder within Trial folder\n",
    "        model_folder = os.path.join(folder_name, 'models')\n",
    "        if not os.path.exists(model_folder):\n",
    "            os.makedirs(model_folder)\n",
    "        file_name = os.path.join(model_folder, 'saved_model_#_LSP_MO_UT' + str(counts))\n",
    "        autoencoder.save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d093fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "#print(X_train_f.index.to_list() ==  y_train_f.index.to_list())\n",
    "#print(X_valid_f.index.to_list() ==  y_valid_f.index.to_list())\n",
    "\n",
    "# Save index list\n",
    "shuffled_index_train = X_train_f.index.to_list()\n",
    "shuffled_index_valid = X_valid_f.index.to_list()\n",
    "\n",
    "# open a binary file in write mode\n",
    "file = open(\"shufftrain\", \"wb\")\n",
    "# save array to the file\n",
    "np.save(file, shuffled_index_train)\n",
    "# close the file\n",
    "file.close\n",
    "# open a binary file in write mode\n",
    "file = open(\"shuffval\", \"wb\")\n",
    "# save array to the file\n",
    "np.save(file, shuffled_index_valid)\n",
    "# close the file\n",
    "file.close"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
