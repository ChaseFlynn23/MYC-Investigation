{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c9d5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-29 21:19:09.449942: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-29 21:19:09.449980: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-29 21:19:09.450012: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-29 21:19:09.458363: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-29 21:19:10.846555: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import glob\n",
    "import sklearn \n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, optimizers\n",
    "from tensorflow.keras.layers import Input, Dense, LeakyReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras_tuner import BayesianOptimization, HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d4b3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import\n",
    "wt_filtered = ['wt_filtered_lcc_3_50.lccdata', 'wt_filtered_lcc_12_50.lccdata', 'wt_filtered_lcc_20_50.lccdata']\n",
    "\n",
    "# filtered wt LCC data import\n",
    "wt_f_var_names = ['wt_3f', 'wt_12f', 'wt_20f']\n",
    "\n",
    "for var, file in zip(wt_f_var_names, wt_filtered):\n",
    "    globals()[var] = pd.read_csv(file, sep='\\t').drop(columns='Unnamed: 0')\n",
    "\n",
    "# filtered mutant LCC data import\n",
    "D132H_filtered = ['D132H_filtered_lcc_3_50.lccdata', 'D132H_filtered_lcc_12_50.lccdata', 'D132H_filtered_lcc_20_50.lccdata']\n",
    "D132H_f_var_names = ['D132H_3f', 'D132H_12f', 'D132H_20f']\n",
    "\n",
    "for var, file in zip(D132H_f_var_names, D132H_filtered):\n",
    "    globals()[var] = pd.read_csv(file, sep='\\t').drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "156a7cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concateneate wt and mutant dataframes and rename columns\n",
    "\n",
    "wt_f = pd.concat([wt_3f, wt_12f, wt_20f], axis = 1)\n",
    "    \n",
    "D132H_f = pd.concat([D132H_3f, D132H_12f, D132H_20f], axis = 1)\n",
    "\n",
    "colnames = [*range(0,12)]\n",
    "colnames\n",
    "wt_f.columns = colnames\n",
    "D132H_f.columns = colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc35f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data pre processing\n",
    "\n",
    "def preprocessing(wt, mutant):\n",
    "    \n",
    "    wt_label = np.zeros(len(wt)) # Set wt labels to 0\n",
    "    \n",
    "    mutant_label = np.ones(len(mutant))\n",
    "    \n",
    "    # Concatenate data frames and label arrays\n",
    "\n",
    "    X_train_full = pd.concat([wt.reset_index(), mutant.reset_index()])\n",
    "    y_train_full = np.concatenate((wt_label, mutant_label))\n",
    "    \n",
    "    #Drop index column and normalise training data\n",
    "    X_train_full = X_train_full.drop(columns = 'index')\n",
    "    \n",
    "    X_train_full= X_train_full.div(100) ## When changed from 100 to 56.1035, errors generated.\n",
    "    \n",
    "    # Separate training and validation sets and print relevant shapes\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, stratify=y_train_full, test_size=0.2)\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(X_valid.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_valid.shape)\n",
    "    \n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f47657e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64000, 12)\n",
      "(16000, 12)\n",
      "(64000,)\n",
      "(16000,)\n"
     ]
    }
   ],
   "source": [
    "X_train_f, X_valid_f, y_train_f, y_valid_f = preprocessing(wt_f, D132H_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dded8413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tracking for the best overall validation loss and hyperparameters\n",
    "best_overall = {\n",
    "    'loss': float('inf'),\n",
    "    'hyperparameters': None,\n",
    "    'step': None,\n",
    "    'details': {}  # A dictionary to store detailed hyperparameters for easy access\n",
    "}\n",
    "\n",
    "# Function to update the overall best with results from a new tuning step\n",
    "def update_best_overall(tuner, step, additional_details=None):\n",
    "    global best_overall\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "    best_loss = tuner.oracle.get_best_trials(1)[0].score  # Adjust based on how you retrieve best score\n",
    "    \n",
    "    # Update if the current step's best loss is better than the global best loss\n",
    "    if best_loss < best_overall['loss']:\n",
    "        best_overall['loss'] = best_loss\n",
    "        best_overall['hyperparameters'] = best_hp\n",
    "        best_overall['step'] = step\n",
    "        if additional_details:\n",
    "            best_overall['details'][step] = additional_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa480db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from AE_Tuning_Asym/AET_Nodes_Layers/tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79404/1395329471.py:3: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import BayesianOptimization\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from kerastuner.tuners import BayesianOptimization\n",
    "\n",
    "def build_autoencoder(hp):\n",
    "    input_shape = X_train_f.shape[1:]  \n",
    "\n",
    "    # Encoder\n",
    "    encoder_input = keras.Input(shape=input_shape)\n",
    "    x = encoder_input\n",
    "    encoder_layers = hp.Int('num_encoder_layers', 1, 5)\n",
    "    for i in range(encoder_layers):  \n",
    "        layer_size = hp.Int(f'encoder_nodes_{i}', min_value=16, max_value=336, step=16)\n",
    "        x = layers.Dense(layer_size, activation=keras.layers.LeakyReLU(alpha=0.01))(x)\n",
    "\n",
    "    # Latent space\n",
    "    latent_space = layers.Dense(2)(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = latent_space\n",
    "    decoder_layers = hp.Int('num_decoder_layers', 1, 5)\n",
    "    for i in range(decoder_layers):\n",
    "        layer_size = hp.Int(f'decoder_nodes_{i}', min_value=16, max_value=336, step=16)\n",
    "        x = layers.Dense(layer_size, activation=keras.layers.LeakyReLU(alpha=0.01))(x)\n",
    "\n",
    "    decoder_output = layers.Dense(input_shape[0], activation='linear')(x)\n",
    "\n",
    "    # Autoencoder model\n",
    "    autoencoder = models.Model(encoder_input, decoder_output)\n",
    "\n",
    "    # Compile model\n",
    "    autoencoder.compile(optimizer=optimizers.Adam(learning_rate=0.005), loss='mse')\n",
    "                        \n",
    "    return autoencoder\n",
    "\n",
    "# Prepare Keras Tuner with Bayesian Optimization\n",
    "tuner = BayesianOptimization(\n",
    "    build_autoencoder,\n",
    "    objective='val_loss',\n",
    "    max_trials=50,\n",
    "    executions_per_trial=2,\n",
    "    num_initial_points=10,\n",
    "    directory='AE_Tuning_Asym',\n",
    "    project_name='AET_Nodes_Layers'\n",
    ")\n",
    "\n",
    "# Start tuning\n",
    "tuner.search(\n",
    "    X_train_f, X_train_f,\n",
    "    epochs=50,\n",
    "    batch_size=256,\n",
    "    validation_data=(X_valid_f, X_valid_f)  \n",
    ")\n",
    "\n",
    "# Retrieve the best hyperparameters after tuning\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# After nodes and layers tuning is complete\n",
    "update_best_overall(tuner, 'nodes_layers', additional_details={\n",
    "    'num_encoder_layers': best_hp.get('num_encoder_layers'),\n",
    "    'encoder_nodes_per_layer': [best_hp.get(f'encoder_nodes_{i}') for i in range(best_hp.get('num_encoder_layers'))],\n",
    "    'num_decoder_layers': best_hp.get('num_decoder_layers'),\n",
    "    'decoder_nodes_per_layer': [best_hp.get(f'decoder_nodes_{i}') for i in range(best_hp.get('num_decoder_layers'))]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f859d7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 12)]              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 176)               2288      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 112)               19824     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 336)               37968     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 674       \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 240)               720       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 12)                2892      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 64366 (251.43 KB)\n",
      "Trainable params: 64366 (251.43 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-29 21:19:13.056134: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# Get the best model and print summary\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d363138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: 1\n",
      "Hyperparameters:\n",
      "num_encoder_layers: 3\n",
      "encoder_nodes_0: 176\n",
      "num_decoder_layers: 1\n",
      "decoder_nodes_0: 240\n",
      "encoder_nodes_1: 112\n",
      "encoder_nodes_2: 336\n",
      "encoder_nodes_3: 192\n",
      "decoder_nodes_1: 336\n",
      "decoder_nodes_2: 224\n",
      "decoder_nodes_3: 176\n",
      "decoder_nodes_4: 256\n",
      "encoder_nodes_4: 80\n",
      "Validation Loss: 0.00036727500264532864\n",
      "\n",
      "Rank: 2\n",
      "Hyperparameters:\n",
      "num_encoder_layers: 3\n",
      "encoder_nodes_0: 16\n",
      "num_decoder_layers: 2\n",
      "decoder_nodes_0: 16\n",
      "encoder_nodes_1: 176\n",
      "encoder_nodes_2: 336\n",
      "encoder_nodes_3: 208\n",
      "decoder_nodes_1: 336\n",
      "decoder_nodes_2: 320\n",
      "decoder_nodes_3: 160\n",
      "decoder_nodes_4: 304\n",
      "encoder_nodes_4: 192\n",
      "Validation Loss: 0.0003689763107104227\n",
      "\n",
      "Rank: 3\n",
      "Hyperparameters:\n",
      "num_encoder_layers: 2\n",
      "encoder_nodes_0: 32\n",
      "num_decoder_layers: 2\n",
      "decoder_nodes_0: 272\n",
      "encoder_nodes_1: 240\n",
      "encoder_nodes_2: 336\n",
      "encoder_nodes_3: 224\n",
      "decoder_nodes_1: 208\n",
      "decoder_nodes_2: 32\n",
      "decoder_nodes_3: 240\n",
      "decoder_nodes_4: 128\n",
      "encoder_nodes_4: 272\n",
      "Validation Loss: 0.0003738460800377652\n",
      "\n",
      "Rank: 4\n",
      "Hyperparameters:\n",
      "num_encoder_layers: 3\n",
      "encoder_nodes_0: 16\n",
      "num_decoder_layers: 3\n",
      "decoder_nodes_0: 320\n",
      "encoder_nodes_1: 208\n",
      "encoder_nodes_2: 128\n",
      "encoder_nodes_3: 80\n",
      "decoder_nodes_1: 304\n",
      "decoder_nodes_2: 64\n",
      "decoder_nodes_3: 80\n",
      "decoder_nodes_4: 208\n",
      "encoder_nodes_4: 112\n",
      "Validation Loss: 0.0003791690105572343\n",
      "\n",
      "Rank: 5\n",
      "Hyperparameters:\n",
      "num_encoder_layers: 3\n",
      "encoder_nodes_0: 32\n",
      "num_decoder_layers: 3\n",
      "decoder_nodes_0: 272\n",
      "encoder_nodes_1: 32\n",
      "encoder_nodes_2: 320\n",
      "encoder_nodes_3: 80\n",
      "decoder_nodes_1: 304\n",
      "decoder_nodes_2: 64\n",
      "decoder_nodes_3: 208\n",
      "decoder_nodes_4: 208\n",
      "encoder_nodes_4: 288\n",
      "Validation Loss: 0.00038073683390393853\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch the full details of the top 5 trials\n",
    "top_trials = tuner.oracle.get_best_trials(num_trials=5)\n",
    "\n",
    "# Print the details of the top 5 trials\n",
    "for i, trial in enumerate(top_trials):\n",
    "    print(f\"Rank: {i+1}\")\n",
    "    print(\"Hyperparameters:\")\n",
    "    for hp, value in trial.hyperparameters.values.items():\n",
    "        print(f\"{hp}: {value}\")\n",
    "    \n",
    "    # Include the validation loss for each model\n",
    "    val_loss = trial.score\n",
    "    print(f\"Validation Loss: {val_loss}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e1cec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from AE_Tuning_Asym/AET_LR_BS/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# Tuning Batch Size and Learning Rate\n",
    "# 'tuner' is the previous tuner instance\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# Retrieve best architecture parameters for both encoder and decoder\n",
    "best_num_encoder_layers = best_hp.get('num_encoder_layers')\n",
    "best_encoder_nodes_per_layer = [best_hp.get(f'encoder_nodes_{i}') for i in range(best_num_encoder_layers)]\n",
    "best_num_decoder_layers = best_hp.get('num_decoder_layers')\n",
    "best_decoder_nodes_per_layer = [best_hp.get(f'decoder_nodes_{i}') for i in range(best_num_decoder_layers)]\n",
    "\n",
    "\n",
    "\n",
    "def build_autoencoder_for_tuning(hp):\n",
    "    input_shape = X_train_f.shape[1:]  \n",
    "\n",
    "     # Encoder: Fixed architecture from previous tuning\n",
    "    encoder_input = keras.Input(shape=input_shape)\n",
    "    x = encoder_input\n",
    "    for layer_size in best_encoder_nodes_per_layer:\n",
    "        x = layers.Dense(layer_size, activation=keras.layers.LeakyReLU(alpha=0.01))(x) \n",
    "\n",
    "    # Latent space\n",
    "    latent_space = layers.Dense(2)(x)\n",
    "\n",
    "    # Decoder: Fixed architecture from previous tuning\n",
    "    x = latent_space\n",
    "    for layer_size in reversed(best_decoder_nodes_per_layer):\n",
    "        x = layers.Dense(layer_size, activation=keras.layers.LeakyReLU(alpha=0.01))(x)  \n",
    "\n",
    "    decoder_output = layers.Dense(input_shape[0], activation='linear')(x)\n",
    "\n",
    "    # Autoencoder model\n",
    "    autoencoder = models.Model(encoder_input, decoder_output)\n",
    "\n",
    "    # Hyperparameters for optimizer: learning rate\n",
    "    lr = hp.Float('learning_rate', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "    batch_size = hp.Int('batch_size', min_value=32, max_value=528, step=16)\n",
    "    \n",
    "    # Compile the model\n",
    "    autoencoder.compile(optimizer=optimizers.Adam(learning_rate=lr), loss='mse')\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "# Set up a new tuner instance for the extended search\n",
    "batch_size_tuner = BayesianOptimization(\n",
    "    build_autoencoder_for_tuning,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    num_initial_points=5,\n",
    "    directory='AE_Tuning_Asym',\n",
    "    project_name='AET_LR_BS'\n",
    ")\n",
    "\n",
    "# Start the new tuning session\n",
    "batch_size_tuner.search(\n",
    "    X_train_f, X_train_f,  \n",
    "    validation_data=(X_valid_f, X_valid_f),  \n",
    "    epochs=50\n",
    ")\n",
    "\n",
    "# Retrieve the best hyperparameters after batch size and learning rate tuning\n",
    "best_batch_lr_hp = batch_size_tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# After batch size and learning rate tuning is complete\n",
    "update_best_overall(batch_size_tuner, 'batch_size_lr', additional_details={\n",
    "    'batch_size': best_batch_lr_hp.get('batch_size'),\n",
    "    'learning_rate': best_batch_lr_hp.get('learning_rate')\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71fc2757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best batch size: 304\n",
      "Best learning rate: 0.00018792358749218356\n"
     ]
    }
   ],
   "source": [
    "# Fetching the best hyperparameters after the search is completed\n",
    "best_hp = batch_size_tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# Retrieve and print best batch size & learning rate\n",
    "best_batch_size = best_hp.get('batch_size')\n",
    "best_learning_rate = best_hp.get('learning_rate')\n",
    "\n",
    "print(f\"Best batch size: {best_batch_size}\")\n",
    "print(f\"Best learning rate: {best_learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4038bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 Complete [00h 00m 39s]\n",
      "val_loss: 0.00043184214155189693\n",
      "\n",
      "Best val_loss So Far: 0.00040717588854022324\n",
      "Total elapsed time: 00h 32m 03s\n"
     ]
    }
   ],
   "source": [
    "# Testing Activation Functions\n",
    "# 'tuner' is tuner instance for architecture\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# 'batch_size_tuner' tuner instance for batch size and learning rate\n",
    "best_batch_lr_hp = batch_size_tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# Retrieve best architecture parameters for both encoder and decoder\n",
    "best_num_encoder_layers = best_hp.get('num_encoder_layers')\n",
    "best_encoder_nodes_per_layer = [best_hp.get(f'encoder_nodes_{i}') for i in range(best_num_encoder_layers)]\n",
    "best_num_decoder_layers = best_hp.get('num_decoder_layers')\n",
    "best_decoder_nodes_per_layer = [best_hp.get(f'decoder_nodes_{i}') for i in range(best_num_decoder_layers)]\n",
    "\n",
    "# Retrieve best batch size and learning rate\n",
    "best_batch_size = best_batch_lr_hp.get('batch_size')\n",
    "best_learning_rate = best_batch_lr_hp.get('learning_rate')\n",
    "\n",
    "\n",
    "def build_autoencoder_for_activation_tuning(hp):\n",
    "    input_shape = X_train_f.shape[1:]  \n",
    "\n",
    "    # Encoder\n",
    "    encoder_input = keras.Input(shape=input_shape)\n",
    "    x = encoder_input\n",
    "    \n",
    "    # Lists to hold activation functions for each layer in the encoder and decoder\n",
    "    encoder_activation_functions = []\n",
    "    decoder_activation_functions = []\n",
    "\n",
    "    # Encoder architecture based on previous tuning\n",
    "    for i in range(best_num_encoder_layers):  \n",
    "        layer_size = best_encoder_nodes_per_layer[i]\n",
    "        # Define hyperparameter for activation function for each layer in the encoder\n",
    "        activation_choice = hp.Choice(f'encoder_activation_{i}', values=['relu', 'sigmoid', 'tanh', 'LeakyReLU'])\n",
    "        encoder_activation_functions.append(activation_choice)\n",
    "\n",
    "        if activation_choice == 'LeakyReLU':\n",
    "            x = layers.Dense(layer_size)(x)\n",
    "            x = layers.LeakyReLU(alpha=0.01)(x)\n",
    "        else:\n",
    "            x = layers.Dense(layer_size, activation=activation_choice)(x)\n",
    "\n",
    "    # Latent space\n",
    "    latent_space = layers.Dense(2)(x)  # Assuming a latent space size of 2\n",
    "\n",
    "    # Decoder (not mirroring the encoder)\n",
    "    x = latent_space\n",
    "    for i in range(best_num_decoder_layers):\n",
    "        layer_size = best_decoder_nodes_per_layer[i]\n",
    "        # Define hyperparameter for activation function for each layer in the decoder\n",
    "        activation_choice = hp.Choice(f'decoder_activation_{i}', values=['relu', 'sigmoid', 'tanh', 'LeakyReLU'])\n",
    "        decoder_activation_functions.append(activation_choice)\n",
    "\n",
    "        if activation_choice == 'LeakyReLU':\n",
    "            x = layers.Dense(layer_size)(x)\n",
    "            x = layers.LeakyReLU(alpha=0.01)(x)\n",
    "        else:\n",
    "            x = layers.Dense(layer_size, activation=activation_choice)(x)\n",
    "\n",
    "    decoder_output = layers.Dense(input_shape[0], activation='linear')(x)\n",
    "\n",
    "    # Autoencoder model\n",
    "    autoencoder = models.Model(encoder_input, decoder_output)\n",
    "\n",
    "    # Compile the model with fixed learning rate from previous tuning\n",
    "    autoencoder.compile(optimizer=optimizers.Adam(learning_rate=best_learning_rate), loss='mse')\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "# Set up a new tuner instance for the activation function search\n",
    "activation_tuner = BayesianOptimization(\n",
    "    build_autoencoder_for_activation_tuning,\n",
    "    objective='val_loss',\n",
    "    max_trials=50,     \n",
    "    executions_per_trial=1,\n",
    "    num_initial_points=10,\n",
    "    directory='AE_Tuning_Asym',\n",
    "    project_name='AET_AF'\n",
    ")\n",
    "\n",
    "# Start the new tuning session with fixed batch size\n",
    "activation_tuner.search(\n",
    "    X_train_f, X_train_f,  \n",
    "    validation_data=(X_valid_f, X_valid_f),  \n",
    "    epochs=50,\n",
    "    batch_size=best_batch_size  # Fixed batch size from previous tuning\n",
    ")\n",
    "\n",
    "# Retrieve the best hyperparameters after activation function tuning\n",
    "best_activation_hp = activation_tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# After activation function tuning is complete\n",
    "update_best_overall(activation_tuner, 'activation', additional_details={\n",
    "    'encoder_activations': [best_activation_hp.get(f'encoder_activation_{i}') for i in range(best_num_encoder_layers)],\n",
    "    'decoder_activations': [best_activation_hp.get(f'decoder_activation_{i}') for i in range(best_num_decoder_layers)]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1236b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 Complete [00h 00m 37s]\n",
      "val_loss: 0.009039172902703285\n",
      "\n",
      "Best val_loss So Far: 0.00042053364450111985\n",
      "Total elapsed time: 00h 05m 56s\n"
     ]
    }
   ],
   "source": [
    "# Tuning Loss Function and Optimizer\n",
    "# 'tuner' is tuner instance for architecture\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# 'batch_size_tuner' is tuner instance for batch size and learning rate\n",
    "best_batch_lr_hp = batch_size_tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# 'activation_tuner' is tuner instance for activation\n",
    "best_activation_hp = activation_tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# Retrieve best architecture hyperparameters for encoder and decoder\n",
    "best_num_encoder_layers = best_hp.get('num_encoder_layers')\n",
    "best_encoder_nodes_per_layer = [best_hp.get(f'encoder_nodes_{i}') for i in range(best_num_encoder_layers)]\n",
    "best_num_decoder_layers = best_hp.get('num_decoder_layers')\n",
    "best_decoder_nodes_per_layer = [best_hp.get(f'decoder_nodes_{i}') for i in range(best_num_decoder_layers)]\n",
    "\n",
    "# Retrieve the best activation functions for each layer from the activation tuner\n",
    "best_encoder_activations = [best_activation_hp.get(f'encoder_activation_{i}') for i in range(best_num_encoder_layers)]\n",
    "best_decoder_activations = [best_activation_hp.get(f'decoder_activation_{i}') for i in range(best_num_decoder_layers)]\n",
    "\n",
    "# Retrieve best batch size and learning rate\n",
    "best_batch_size = best_batch_lr_hp.get('batch_size')\n",
    "best_learning_rate = best_batch_lr_hp.get('learning_rate')\n",
    "\n",
    "def build_autoencoder_for_final_tuning(hp):\n",
    "    input_shape = X_train_f.shape[1:]\n",
    "\n",
    "    # Encoder\n",
    "    encoder_input = keras.Input(shape=input_shape)\n",
    "    x = encoder_input\n",
    "    \n",
    "    # Encoder architecture based on previous tuning, with respective activation functions\n",
    "    for i in range(best_num_encoder_layers):  \n",
    "        layer_size = best_encoder_nodes_per_layer[i]\n",
    "        activation_choice = best_encoder_activations[i]  # Use best activation from the activation tuner\n",
    "\n",
    "        if activation_choice == 'LeakyReLU':\n",
    "            x = layers.Dense(layer_size)(x)\n",
    "            x = layers.LeakyReLU(alpha=0.01)(x)\n",
    "        else:\n",
    "            x = layers.Dense(layer_size, activation=activation_choice)(x)\n",
    "\n",
    "    # Latent space\n",
    "    latent_space = layers.Dense(2)(x)\n",
    "\n",
    "    # Decoder (not mirroring the encoder)\n",
    "    x = latent_space\n",
    "    for i in range(best_num_decoder_layers):\n",
    "        layer_size = best_decoder_nodes_per_layer[i]\n",
    "        activation_choice = best_decoder_activations[i]  # Use best activation from the activation tuner for decoder\n",
    "\n",
    "        if activation_choice == 'LeakyReLU':\n",
    "            x = layers.Dense(layer_size)(x)\n",
    "            x = layers.LeakyReLU(alpha=0.01)(x)\n",
    "        else:\n",
    "            x = layers.Dense(layer_size, activation=activation_choice)(x)\n",
    "\n",
    "    decoder_output = layers.Dense(input_shape[0], activation='linear')(x)\n",
    "\n",
    "    # Autoencoder model\n",
    "    autoencoder = models.Model(encoder_input, decoder_output)\n",
    "\n",
    "    # Hyperparameters for optimizer and loss function\n",
    "    optimizer_name = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop'])\n",
    "    loss_function = hp.Choice('loss_function', values=['mse', 'binary_crossentropy', 'mae'])\n",
    "\n",
    "    # Compile the model with hyperparameters\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = optimizers.Adam(learning_rate=best_learning_rate)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        optimizer = optimizers.SGD(learning_rate=best_learning_rate)\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        optimizer = optimizers.RMSprop(learning_rate=best_learning_rate)\n",
    "    \n",
    "    autoencoder.compile(optimizer=optimizer, loss=loss_function)\n",
    "\n",
    "    return autoencoder\n",
    "\n",
    "# New tuner instance for the loss function and optimizer search\n",
    "final_tuner = BayesianOptimization(\n",
    "    build_autoencoder_for_final_tuning,\n",
    "    objective='val_loss',\n",
    "    max_trials=50,\n",
    "    executions_per_trial=1,\n",
    "    num_initial_points=10,\n",
    "    directory='AE_Tuning_Asym',\n",
    "    project_name='AET_LF_Opt'\n",
    ")\n",
    "\n",
    "# Start new tuning session with fixed batch size\n",
    "final_tuner.search(\n",
    "    X_train_f, X_train_f,\n",
    "    validation_data=(X_valid_f, X_valid_f),\n",
    "    epochs=50,\n",
    "    batch_size=best_batch_size  # Fixed batch size from previous tuning\n",
    ")\n",
    "\n",
    "# Retrieve the best hyperparameters after loss function and optimizer tuning\n",
    "best_final_hyperparams = final_tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# After loss function and optimizer tuning is complete\n",
    "update_best_overall(final_tuner, 'loss_optimizer', additional_details={\n",
    "    'loss_function': best_final_hyperparams.get('loss_function'),\n",
    "    'optimizer': best_final_hyperparams.get('optimizer')\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1988f0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Loss Function: mse\n",
      "Best Optimizer: adam\n"
     ]
    }
   ],
   "source": [
    "# Fetch the best hyperparameters after the final tuning session\n",
    "best_final_hp = final_tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "# Retrieve the best loss function and optimizer\n",
    "best_loss_function = best_final_hp.get('loss_function')\n",
    "best_optimizer = best_final_hp.get('optimizer')\n",
    "\n",
    "# Print the results\n",
    "print(f\"Best Loss Function: {best_loss_function}\")\n",
    "print(f\"Best Optimizer: {best_optimizer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b560c3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best global validation loss achieved at step: batch_size_lr\n",
      "With Validation Loss: 0.0003387618635315448\n",
      "Best Hyperparameters:\n",
      "batch_size: 304\n",
      "learning_rate: 0.00018792358749218356\n"
     ]
    }
   ],
   "source": [
    "# Print Best Global Validation Loss and Hyperparameters\n",
    "print(f\"Best global validation loss achieved at step: {best_overall['step']}\")\n",
    "print(f\"With Validation Loss: {best_overall['loss']}\")\n",
    "\n",
    "# Printing details depending on the step\n",
    "if best_overall['step'] in best_overall['details']:\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    for key, value in best_overall['details'][best_overall['step']].items():\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847497a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
