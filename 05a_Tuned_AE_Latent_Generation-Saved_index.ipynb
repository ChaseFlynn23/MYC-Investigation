{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09327dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 14:43:15.524261: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-24 14:43:15.524298: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-24 14:43:15.524322: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-24 14:43:15.531865: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-24 14:43:16.571383: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "import glob\n",
    "import sklearn \n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, optimizers\n",
    "from tensorflow.keras.layers import Input, Activation, Dense, LeakyReLU\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras_tuner import BayesianOptimization, HyperParameters\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5e0bf33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WT for window size = 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>20</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.3249</td>\n",
       "      <td>9.6718</td>\n",
       "      <td>6.0082</td>\n",
       "      <td>9.6574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3105</td>\n",
       "      <td>10.6216</td>\n",
       "      <td>5.9996</td>\n",
       "      <td>11.2509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.7233</td>\n",
       "      <td>11.4243</td>\n",
       "      <td>7.3021</td>\n",
       "      <td>11.5329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.4864</td>\n",
       "      <td>10.7079</td>\n",
       "      <td>6.4156</td>\n",
       "      <td>10.7262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.4673</td>\n",
       "      <td>12.0023</td>\n",
       "      <td>6.3573</td>\n",
       "      <td>11.2367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>4.9712</td>\n",
       "      <td>8.9677</td>\n",
       "      <td>10.1179</td>\n",
       "      <td>8.9430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>4.8021</td>\n",
       "      <td>9.0623</td>\n",
       "      <td>10.1026</td>\n",
       "      <td>8.6752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>5.9590</td>\n",
       "      <td>10.3438</td>\n",
       "      <td>8.6309</td>\n",
       "      <td>9.4556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>4.3996</td>\n",
       "      <td>10.3760</td>\n",
       "      <td>9.5901</td>\n",
       "      <td>9.3513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>5.2675</td>\n",
       "      <td>10.8606</td>\n",
       "      <td>8.6371</td>\n",
       "      <td>9.7884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            7       20       26       27\n",
       "0      6.3249   9.6718   6.0082   9.6574\n",
       "1      6.3105  10.6216   5.9996  11.2509\n",
       "2      5.7233  11.4243   7.3021  11.5329\n",
       "3      5.4864  10.7079   6.4156  10.7262\n",
       "4      5.4673  12.0023   6.3573  11.2367\n",
       "...       ...      ...      ...      ...\n",
       "39995  4.9712   8.9677  10.1179   8.9430\n",
       "39996  4.8021   9.0623  10.1026   8.6752\n",
       "39997  5.9590  10.3438   8.6309   9.4556\n",
       "39998  4.3996  10.3760   9.5901   9.3513\n",
       "39999  5.2675  10.8606   8.6371   9.7884\n",
       "\n",
       "[40000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WT for window size = 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>19</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.9702</td>\n",
       "      <td>17.3049</td>\n",
       "      <td>22.7152</td>\n",
       "      <td>30.7508</td>\n",
       "      <td>24.7567</td>\n",
       "      <td>23.1634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.2704</td>\n",
       "      <td>22.4792</td>\n",
       "      <td>22.4227</td>\n",
       "      <td>28.1640</td>\n",
       "      <td>23.8363</td>\n",
       "      <td>23.2798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.8551</td>\n",
       "      <td>25.5636</td>\n",
       "      <td>24.6903</td>\n",
       "      <td>30.2936</td>\n",
       "      <td>24.8655</td>\n",
       "      <td>23.4604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.3881</td>\n",
       "      <td>26.4733</td>\n",
       "      <td>24.4458</td>\n",
       "      <td>29.5746</td>\n",
       "      <td>21.3842</td>\n",
       "      <td>22.6630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.2788</td>\n",
       "      <td>24.0147</td>\n",
       "      <td>23.2922</td>\n",
       "      <td>27.5496</td>\n",
       "      <td>23.8249</td>\n",
       "      <td>22.3185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>19.7170</td>\n",
       "      <td>14.7258</td>\n",
       "      <td>12.9418</td>\n",
       "      <td>8.4287</td>\n",
       "      <td>24.3534</td>\n",
       "      <td>22.6400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>17.8741</td>\n",
       "      <td>15.5602</td>\n",
       "      <td>12.3054</td>\n",
       "      <td>7.1757</td>\n",
       "      <td>24.9417</td>\n",
       "      <td>24.1508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>17.2324</td>\n",
       "      <td>15.4532</td>\n",
       "      <td>12.3721</td>\n",
       "      <td>7.1490</td>\n",
       "      <td>23.8513</td>\n",
       "      <td>27.4276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>17.0120</td>\n",
       "      <td>15.1188</td>\n",
       "      <td>13.4625</td>\n",
       "      <td>7.5725</td>\n",
       "      <td>25.4096</td>\n",
       "      <td>26.8701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>16.6571</td>\n",
       "      <td>15.4409</td>\n",
       "      <td>13.2021</td>\n",
       "      <td>7.2976</td>\n",
       "      <td>26.2683</td>\n",
       "      <td>24.6460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             6       16       17       19       29       30\n",
       "0      14.9702  17.3049  22.7152  30.7508  24.7567  23.1634\n",
       "1      13.2704  22.4792  22.4227  28.1640  23.8363  23.2798\n",
       "2      13.8551  25.5636  24.6903  30.2936  24.8655  23.4604\n",
       "3      12.3881  26.4733  24.4458  29.5746  21.3842  22.6630\n",
       "4      14.2788  24.0147  23.2922  27.5496  23.8249  22.3185\n",
       "...        ...      ...      ...      ...      ...      ...\n",
       "39995  19.7170  14.7258  12.9418   8.4287  24.3534  22.6400\n",
       "39996  17.8741  15.5602  12.3054   7.1757  24.9417  24.1508\n",
       "39997  17.2324  15.4532  12.3721   7.1490  23.8513  27.4276\n",
       "39998  17.0120  15.1188  13.4625   7.5725  25.4096  26.8701\n",
       "39999  16.6571  15.4409  13.2021   7.2976  26.2683  24.6460\n",
       "\n",
       "[40000 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WT for window size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>9</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.2374</td>\n",
       "      <td>40.1886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.5602</td>\n",
       "      <td>36.3229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.4329</td>\n",
       "      <td>41.2223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.1207</td>\n",
       "      <td>39.2809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.6335</td>\n",
       "      <td>38.7009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>16.9755</td>\n",
       "      <td>31.9599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>17.0192</td>\n",
       "      <td>29.9824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>18.5272</td>\n",
       "      <td>29.4126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>17.1766</td>\n",
       "      <td>32.6983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>17.7794</td>\n",
       "      <td>33.5279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             9       21\n",
       "0      14.2374  40.1886\n",
       "1      14.5602  36.3229\n",
       "2      17.4329  41.2223\n",
       "3      20.1207  39.2809\n",
       "4      15.6335  38.7009\n",
       "...        ...      ...\n",
       "39995  16.9755  31.9599\n",
       "39996  17.0192  29.9824\n",
       "39997  18.5272  29.4126\n",
       "39998  17.1766  32.6983\n",
       "39999  17.7794  33.5279\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---------------------------------\n",
      "D132H for window size = 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>20</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.6253</td>\n",
       "      <td>11.4558</td>\n",
       "      <td>7.0366</td>\n",
       "      <td>10.8265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.6219</td>\n",
       "      <td>11.7791</td>\n",
       "      <td>6.8272</td>\n",
       "      <td>10.8156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.3355</td>\n",
       "      <td>9.9860</td>\n",
       "      <td>6.2585</td>\n",
       "      <td>9.7506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.7119</td>\n",
       "      <td>8.4264</td>\n",
       "      <td>5.8994</td>\n",
       "      <td>8.1239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.7166</td>\n",
       "      <td>9.3508</td>\n",
       "      <td>6.4404</td>\n",
       "      <td>10.1599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>4.6642</td>\n",
       "      <td>10.3721</td>\n",
       "      <td>10.9673</td>\n",
       "      <td>9.8788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>4.7521</td>\n",
       "      <td>11.6737</td>\n",
       "      <td>10.7638</td>\n",
       "      <td>9.5196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>4.5160</td>\n",
       "      <td>9.7824</td>\n",
       "      <td>10.9669</td>\n",
       "      <td>9.6701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>4.8358</td>\n",
       "      <td>7.8161</td>\n",
       "      <td>10.7679</td>\n",
       "      <td>9.6355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>4.5663</td>\n",
       "      <td>10.1266</td>\n",
       "      <td>10.9913</td>\n",
       "      <td>9.2231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            7       20       26       27\n",
       "0      5.6253  11.4558   7.0366  10.8265\n",
       "1      5.6219  11.7791   6.8272  10.8156\n",
       "2      6.3355   9.9860   6.2585   9.7506\n",
       "3      4.7119   8.4264   5.8994   8.1239\n",
       "4      7.7166   9.3508   6.4404  10.1599\n",
       "...       ...      ...      ...      ...\n",
       "39995  4.6642  10.3721  10.9673   9.8788\n",
       "39996  4.7521  11.6737  10.7638   9.5196\n",
       "39997  4.5160   9.7824  10.9669   9.6701\n",
       "39998  4.8358   7.8161  10.7679   9.6355\n",
       "39999  4.5663  10.1266  10.9913   9.2231\n",
       "\n",
       "[40000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D132H for window size = 12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>19</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.9958</td>\n",
       "      <td>16.3721</td>\n",
       "      <td>20.6416</td>\n",
       "      <td>29.6048</td>\n",
       "      <td>24.1730</td>\n",
       "      <td>23.9425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0499</td>\n",
       "      <td>15.4543</td>\n",
       "      <td>20.4593</td>\n",
       "      <td>28.2314</td>\n",
       "      <td>25.1477</td>\n",
       "      <td>23.0454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.5011</td>\n",
       "      <td>13.1972</td>\n",
       "      <td>16.8349</td>\n",
       "      <td>29.2049</td>\n",
       "      <td>23.6181</td>\n",
       "      <td>18.1444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.6412</td>\n",
       "      <td>15.6381</td>\n",
       "      <td>18.6334</td>\n",
       "      <td>27.5069</td>\n",
       "      <td>25.6890</td>\n",
       "      <td>20.3013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.5941</td>\n",
       "      <td>18.8374</td>\n",
       "      <td>20.8179</td>\n",
       "      <td>29.7816</td>\n",
       "      <td>23.7772</td>\n",
       "      <td>20.6936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>13.3469</td>\n",
       "      <td>15.4144</td>\n",
       "      <td>13.0795</td>\n",
       "      <td>12.4362</td>\n",
       "      <td>7.2937</td>\n",
       "      <td>12.2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>13.3072</td>\n",
       "      <td>15.0391</td>\n",
       "      <td>12.4597</td>\n",
       "      <td>12.2133</td>\n",
       "      <td>7.2107</td>\n",
       "      <td>13.3563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>13.4729</td>\n",
       "      <td>15.5238</td>\n",
       "      <td>12.5300</td>\n",
       "      <td>12.2077</td>\n",
       "      <td>7.7544</td>\n",
       "      <td>12.6172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>12.8318</td>\n",
       "      <td>15.9725</td>\n",
       "      <td>13.9111</td>\n",
       "      <td>12.2538</td>\n",
       "      <td>7.3585</td>\n",
       "      <td>12.1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>13.2777</td>\n",
       "      <td>15.6850</td>\n",
       "      <td>13.5851</td>\n",
       "      <td>12.3831</td>\n",
       "      <td>7.2522</td>\n",
       "      <td>12.8442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             6       16       17       19       29       30\n",
       "0      14.9958  16.3721  20.6416  29.6048  24.1730  23.9425\n",
       "1      15.0499  15.4543  20.4593  28.2314  25.1477  23.0454\n",
       "2      16.5011  13.1972  16.8349  29.2049  23.6181  18.1444\n",
       "3      13.6412  15.6381  18.6334  27.5069  25.6890  20.3013\n",
       "4      16.5941  18.8374  20.8179  29.7816  23.7772  20.6936\n",
       "...        ...      ...      ...      ...      ...      ...\n",
       "39995  13.3469  15.4144  13.0795  12.4362   7.2937  12.2014\n",
       "39996  13.3072  15.0391  12.4597  12.2133   7.2107  13.3563\n",
       "39997  13.4729  15.5238  12.5300  12.2077   7.7544  12.6172\n",
       "39998  12.8318  15.9725  13.9111  12.2538   7.3585  12.1032\n",
       "39999  13.2777  15.6850  13.5851  12.3831   7.2522  12.8442\n",
       "\n",
       "[40000 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D132H for window size = 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>9</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13.4055</td>\n",
       "      <td>41.6856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.0150</td>\n",
       "      <td>40.2262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.0081</td>\n",
       "      <td>41.7220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.6520</td>\n",
       "      <td>40.4408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.7450</td>\n",
       "      <td>38.0932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>14.0324</td>\n",
       "      <td>23.2605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>14.9455</td>\n",
       "      <td>22.4160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>14.0516</td>\n",
       "      <td>22.7481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>14.0350</td>\n",
       "      <td>23.5958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>14.3382</td>\n",
       "      <td>22.8859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             9       21\n",
       "0      13.4055  41.6856\n",
       "1      12.0150  40.2262\n",
       "2      17.0081  41.7220\n",
       "3      15.6520  40.4408\n",
       "4      20.7450  38.0932\n",
       "...        ...      ...\n",
       "39995  14.0324  23.2605\n",
       "39996  14.9455  22.4160\n",
       "39997  14.0516  22.7481\n",
       "39998  14.0350  23.5958\n",
       "39999  14.3382  22.8859\n",
       "\n",
       "[40000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data import\n",
    "wt_filtered = ['wt_filtered_lcc_3_50.lccdata', 'wt_filtered_lcc_12_50.lccdata', 'wt_filtered_lcc_20_50.lccdata']\n",
    "\n",
    "# filtered wt LCC data import\n",
    "wt_f_var_names = ['wt_3f', 'wt_12f', 'wt_20f']\n",
    "\n",
    "for var, file in zip(wt_f_var_names, wt_filtered):\n",
    "    globals()[var] = pd.read_csv(file, sep='\\t').drop(columns='Unnamed: 0')\n",
    "\n",
    "# filtered mutant LCC data import\n",
    "D132H_filtered = ['D132H_filtered_lcc_3_50.lccdata', 'D132H_filtered_lcc_12_50.lccdata', 'D132H_filtered_lcc_20_50.lccdata']\n",
    "D132H_f_var_names = ['D132H_3f', 'D132H_12f', 'D132H_20f']\n",
    "\n",
    "for var, file in zip(D132H_f_var_names, D132H_filtered):\n",
    "    globals()[var] = pd.read_csv(file, sep='\\t').drop(columns='Unnamed: 0')\n",
    "    \n",
    "# Visualization of dataset\n",
    "print('WT for window size = 3')\n",
    "display(wt_3f)\n",
    "print('WT for window size = 12')\n",
    "display(wt_12f)\n",
    "print('WT for window size = 20')\n",
    "display(wt_20f)\n",
    "\n",
    "print('\\n')\n",
    "print('---------------------------------')\n",
    "print('D132H for window size = 3')\n",
    "display(D132H_3f)\n",
    "print('D132H for window size = 12')\n",
    "display(D132H_12f)\n",
    "print('D132H for window size = 20')\n",
    "display(D132H_20f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc552490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concateneate wt and mutant dataframes and rename columns\n",
    "\n",
    "wt_f = pd.concat([wt_3f, wt_12f, wt_20f], axis = 1)\n",
    "    \n",
    "D132H_f = pd.concat([D132H_3f, D132H_12f, D132H_20f], axis = 1)\n",
    "\n",
    "colnames = [*range(0,12)]\n",
    "colnames\n",
    "wt_f.columns = colnames\n",
    "D132H_f.columns = colnames"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42257098",
   "metadata": {},
   "source": [
    "# Data pre processing\n",
    "\n",
    "def preprocessing(wt, mutant):\n",
    "    \n",
    "    wt_label = np.zeros(len(wt)) # Set wt labels to 0\n",
    "    \n",
    "    mutant_label = np.ones(len(mutant))\n",
    "    \n",
    "    # Concatenate data frames and label arrays\n",
    "\n",
    "    X_train_full = pd.concat([wt.reset_index(), mutant.reset_index()])\n",
    "    y_train_full = np.concatenate((wt_label, mutant_label))\n",
    "    \n",
    "    #Drop index column and normalise training data\n",
    "    X_train_full = X_train_full.drop(columns = 'index')\n",
    "    \n",
    "    X_train_full= X_train_full.div(100) ## When changed from 100 to 56.1035, errors generated.\n",
    "    \n",
    "    # Separate training and validation sets and print relevant shapes\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, stratify=y_train_full, test_size=0.2)\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(X_valid.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_valid.shape)\n",
    "    \n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa0b2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(wt, mutant):\n",
    "    \n",
    "    wt_label = np.zeros(len(wt))  # Set wt labels to 0\n",
    "    mutant_label = np.ones(len(mutant))  # Set mutant labels to 1\n",
    "\n",
    "    # Create label dataframes with indices\n",
    "    wt_label_df = pd.DataFrame({'class': wt_label})\n",
    "    mutant_label_df = pd.DataFrame({'class': mutant_label})\n",
    "\n",
    "    # Concatenate data frames and label dataframes\n",
    "    X_train_full = pd.concat([wt, mutant])\n",
    "    y_train_full_df = pd.concat([wt_label_df, mutant_label_df])\n",
    "\n",
    "    # Normalize training data\n",
    "    X_train_full = X_train_full.div(100)  # Adjust as necessary\n",
    "\n",
    "    # Separate training and validation sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full_df, stratify=y_train_full_df['class'], test_size=0.2)\n",
    "\n",
    "    print(X_train.shape)\n",
    "    print(X_valid.shape)\n",
    "    print(y_train.shape)\n",
    "    print(y_valid.shape)\n",
    "\n",
    "    return X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3072ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64000, 12)\n",
      "(16000, 12)\n",
      "(64000, 1)\n",
      "(16000, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train_f, X_valid_f, y_train_f, y_valid_f = preprocessing(wt_f, D132H_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f3ed4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       class\n",
      "20172    1.0\n",
      "13523    0.0\n",
      "4203     1.0\n",
      "29611    0.0\n",
      "30581    0.0\n",
      "...      ...\n",
      "16698    1.0\n",
      "1172     1.0\n",
      "22512    1.0\n",
      "29130    1.0\n",
      "8114     1.0\n",
      "\n",
      "[64000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(y_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f91c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get autoencoder model\n",
    "def get_ae(train_data, LeReLU_alpha=0.01):\n",
    "    \n",
    "    #Input layer\n",
    "    input_layer = Input(shape=(train_data.shape[1]), name='ae_input')\n",
    "    \n",
    "    encoder = Dense(336, activation=LeakyReLU(alpha=LeReLU_alpha), name='e1')(input_layer)\n",
    "    encoder = Dense(208, activation=LeakyReLU(alpha=LeReLU_alpha), name='e2')(encoder)\n",
    "    encoder = Dense(240, activation=LeakyReLU(alpha=LeReLU_alpha), name='e3')(encoder)\n",
    "\n",
    "    encoded = Dense(2, activation=LeakyReLU(alpha=LeReLU_alpha), name='ae_latent')(encoder)\n",
    "    \n",
    "    decoder = Dense(240, activation=LeakyReLU(alpha=LeReLU_alpha), name='d1')(encoded)\n",
    "    decoder = Dense(208, activation=LeakyReLU(alpha=LeReLU_alpha), name='d2')(decoder)\n",
    "    decoder = Dense(336, activation=LeakyReLU(alpha=LeReLU_alpha), name='d3')(decoder)\n",
    "\n",
    "    output_layer = Dense(train_data.shape[1], activation=LeakyReLU(alpha=LeReLU_alpha), name='ae_output')(decoder)\n",
    "    \n",
    "    model = Model(input_layer, output_layer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78f15476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 14:47:37.793453: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# Get ae for filtered data\n",
    "autoencoder = get_ae(X_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0364035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " ae_input (InputLayer)       [(None, 12)]              0         \n",
      "                                                                 \n",
      " e1 (Dense)                  (None, 336)               4368      \n",
      "                                                                 \n",
      " e2 (Dense)                  (None, 208)               70096     \n",
      "                                                                 \n",
      " e3 (Dense)                  (None, 240)               50160     \n",
      "                                                                 \n",
      " ae_latent (Dense)           (None, 2)                 482       \n",
      "                                                                 \n",
      " d1 (Dense)                  (None, 240)               720       \n",
      "                                                                 \n",
      " d2 (Dense)                  (None, 208)               50128     \n",
      "                                                                 \n",
      " d3 (Dense)                  (None, 336)               70224     \n",
      "                                                                 \n",
      " ae_output (Dense)           (None, 12)                4044      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 250222 (977.43 KB)\n",
      "Trainable params: 250222 (977.43 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print summary of ae model\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a20e45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "autoencoder.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam(learning_rate = 0.00001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1088d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = \"AET_CF_Trial_12\"\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "name = \"12_LSP_AET_CF_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4ac9dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f.to_csv(f'{folder_name}/X_train_f.csv')\n",
    "y_train_f.to_csv(f'{folder_name}/y_train_f.csv')\n",
    "\n",
    "X_valid_f.to_csv(f'{folder_name}/X_valid_f.csv')\n",
    "y_valid_f.to_csv(f'{folder_name}/y_valid_f.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe077b70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.0177 - val_loss: 0.0096\n",
      "Epoch 2/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.0069 - val_loss: 0.0064\n",
      "Epoch 3/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.0051 - val_loss: 0.0043\n",
      "Epoch 4/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.0031 - val_loss: 0.0015\n",
      "Epoch 5/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 6/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 7/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 8/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 9/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 10/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 11/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 12/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 13/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 14/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 15/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 16/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 17/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 18/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 19/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 20/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 21/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 22/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 23/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.0011 - val_loss: 8.3917e-04\n",
      "Epoch 24/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 7.0464e-04 - val_loss: 6.7696e-04\n",
      "Epoch 25/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 6.6447e-04 - val_loss: 6.6472e-04\n",
      "Epoch 26/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 6.5583e-04 - val_loss: 6.5782e-04\n",
      "Epoch 27/1000\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 6.5031e-04 - val_loss: 6.5277e-04\n",
      "Epoch 28/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 6.4566e-04 - val_loss: 6.4813e-04\n",
      "Epoch 29/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 6.4112e-04 - val_loss: 6.4347e-04\n",
      "Epoch 30/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 6.3674e-04 - val_loss: 6.3943e-04\n",
      "Epoch 31/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 6.3292e-04 - val_loss: 6.3515e-04\n",
      "Epoch 32/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 6.2909e-04 - val_loss: 6.3171e-04\n",
      "Epoch 33/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 6.2557e-04 - val_loss: 6.2831e-04\n",
      "Epoch 34/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 6.2253e-04 - val_loss: 6.2501e-04\n",
      "Epoch 35/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 6.1950e-04 - val_loss: 6.2166e-04\n",
      "Epoch 36/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 6.1662e-04 - val_loss: 6.1937e-04\n",
      "Epoch 37/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 6.1383e-04 - val_loss: 6.1593e-04\n",
      "Epoch 38/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 6.1125e-04 - val_loss: 6.1305e-04\n",
      "Epoch 39/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 6.0845e-04 - val_loss: 6.1040e-04\n",
      "Epoch 40/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 6.0582e-04 - val_loss: 6.0732e-04\n",
      "Epoch 41/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 6.0304e-04 - val_loss: 6.0431e-04\n",
      "Epoch 42/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 6.0012e-04 - val_loss: 6.0206e-04\n",
      "Epoch 43/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.9724e-04 - val_loss: 5.9791e-04\n",
      "Epoch 44/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.9375e-04 - val_loss: 5.9360e-04\n",
      "Epoch 45/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.8985e-04 - val_loss: 5.8966e-04\n",
      "Epoch 46/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.8601e-04 - val_loss: 5.8520e-04\n",
      "Epoch 47/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.8192e-04 - val_loss: 5.8062e-04\n",
      "Epoch 48/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.7737e-04 - val_loss: 5.7600e-04\n",
      "Epoch 49/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.7284e-04 - val_loss: 5.7036e-04\n",
      "Epoch 50/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.6792e-04 - val_loss: 5.6542e-04\n",
      "Epoch 51/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.6307e-04 - val_loss: 5.6053e-04\n",
      "Epoch 52/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.5850e-04 - val_loss: 5.5559e-04\n",
      "Epoch 53/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.5474e-04 - val_loss: 5.5226e-04\n",
      "Epoch 54/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.5132e-04 - val_loss: 5.4879e-04\n",
      "Epoch 55/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.4829e-04 - val_loss: 5.4602e-04\n",
      "Epoch 56/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.4568e-04 - val_loss: 5.4349e-04\n",
      "Epoch 57/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.4336e-04 - val_loss: 5.4117e-04\n",
      "Epoch 58/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.4119e-04 - val_loss: 5.3978e-04\n",
      "Epoch 59/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.3926e-04 - val_loss: 5.3734e-04\n",
      "Epoch 60/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.3739e-04 - val_loss: 5.3548e-04\n",
      "Epoch 61/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.3567e-04 - val_loss: 5.3429e-04\n",
      "Epoch 62/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.3410e-04 - val_loss: 5.3229e-04\n",
      "Epoch 63/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.3236e-04 - val_loss: 5.3155e-04\n",
      "Epoch 64/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.3079e-04 - val_loss: 5.2928e-04\n",
      "Epoch 65/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.2927e-04 - val_loss: 5.2774e-04\n",
      "Epoch 66/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.2778e-04 - val_loss: 5.2695e-04\n",
      "Epoch 67/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.2638e-04 - val_loss: 5.2491e-04\n",
      "Epoch 68/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.2486e-04 - val_loss: 5.2377e-04\n",
      "Epoch 69/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.2339e-04 - val_loss: 5.2208e-04\n",
      "Epoch 70/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.2204e-04 - val_loss: 5.2099e-04\n",
      "Epoch 71/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.2060e-04 - val_loss: 5.1942e-04\n",
      "Epoch 72/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.1924e-04 - val_loss: 5.1769e-04\n",
      "Epoch 73/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.1774e-04 - val_loss: 5.1660e-04\n",
      "Epoch 74/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.1627e-04 - val_loss: 5.1493e-04\n",
      "Epoch 75/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.1493e-04 - val_loss: 5.1350e-04\n",
      "Epoch 76/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 5.1364e-04 - val_loss: 5.1224e-04\n",
      "Epoch 77/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 5.1223e-04 - val_loss: 5.1119e-04\n",
      "Epoch 78/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 5.1098e-04 - val_loss: 5.0977e-04\n",
      "Epoch 79/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 5.0959e-04 - val_loss: 5.0834e-04\n",
      "Epoch 80/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 5.0844e-04 - val_loss: 5.0686e-04\n",
      "Epoch 81/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 5.0707e-04 - val_loss: 5.0615e-04\n",
      "Epoch 82/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 5.0605e-04 - val_loss: 5.0488e-04\n",
      "Epoch 83/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 5.0477e-04 - val_loss: 5.0323e-04\n",
      "Epoch 84/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 5.0363e-04 - val_loss: 5.0235e-04\n",
      "Epoch 85/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 5.0247e-04 - val_loss: 5.0138e-04\n",
      "Epoch 86/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 5.0132e-04 - val_loss: 4.9979e-04\n",
      "Epoch 87/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 5.0025e-04 - val_loss: 4.9911e-04\n",
      "Epoch 88/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.9914e-04 - val_loss: 4.9806e-04\n",
      "Epoch 89/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.9808e-04 - val_loss: 4.9628e-04\n",
      "Epoch 90/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.9712e-04 - val_loss: 4.9552e-04\n",
      "Epoch 91/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.9598e-04 - val_loss: 4.9516e-04\n",
      "Epoch 92/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.9499e-04 - val_loss: 4.9390e-04\n",
      "Epoch 93/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.9397e-04 - val_loss: 4.9252e-04\n",
      "Epoch 94/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.9303e-04 - val_loss: 4.9127e-04\n",
      "Epoch 95/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.9197e-04 - val_loss: 4.9053e-04\n",
      "Epoch 96/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.9101e-04 - val_loss: 4.8939e-04\n",
      "Epoch 97/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.9008e-04 - val_loss: 4.8817e-04\n",
      "Epoch 98/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.8916e-04 - val_loss: 4.8778e-04\n",
      "Epoch 99/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.8811e-04 - val_loss: 4.8651e-04\n",
      "Epoch 100/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.8720e-04 - val_loss: 4.8580e-04\n",
      "Epoch 101/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.8627e-04 - val_loss: 4.8489e-04\n",
      "Epoch 102/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.8540e-04 - val_loss: 4.8397e-04\n",
      "Epoch 103/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.8448e-04 - val_loss: 4.8266e-04\n",
      "Epoch 104/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.8357e-04 - val_loss: 4.8229e-04\n",
      "Epoch 105/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.8263e-04 - val_loss: 4.8100e-04\n",
      "Epoch 106/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.8177e-04 - val_loss: 4.8040e-04\n",
      "Epoch 107/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.8091e-04 - val_loss: 4.7927e-04\n",
      "Epoch 108/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.8019e-04 - val_loss: 4.7886e-04\n",
      "Epoch 109/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.7949e-04 - val_loss: 4.7833e-04\n",
      "Epoch 110/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.7857e-04 - val_loss: 4.7756e-04\n",
      "Epoch 111/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.7776e-04 - val_loss: 4.7646e-04\n",
      "Epoch 112/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.7696e-04 - val_loss: 4.7557e-04\n",
      "Epoch 113/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.7631e-04 - val_loss: 4.7494e-04\n",
      "Epoch 114/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.7557e-04 - val_loss: 4.7391e-04\n",
      "Epoch 115/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.7473e-04 - val_loss: 4.7348e-04\n",
      "Epoch 116/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.7412e-04 - val_loss: 4.7276e-04\n",
      "Epoch 117/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.7334e-04 - val_loss: 4.7239e-04\n",
      "Epoch 118/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.7272e-04 - val_loss: 4.7120e-04\n",
      "Epoch 119/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.7197e-04 - val_loss: 4.7067e-04\n",
      "Epoch 120/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.7131e-04 - val_loss: 4.7021e-04\n",
      "Epoch 121/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.7063e-04 - val_loss: 4.6934e-04\n",
      "Epoch 122/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.7005e-04 - val_loss: 4.6927e-04\n",
      "Epoch 123/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.6937e-04 - val_loss: 4.6867e-04\n",
      "Epoch 124/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.6885e-04 - val_loss: 4.6769e-04\n",
      "Epoch 125/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.6817e-04 - val_loss: 4.6681e-04\n",
      "Epoch 126/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.6760e-04 - val_loss: 4.6638e-04\n",
      "Epoch 127/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.6697e-04 - val_loss: 4.6609e-04\n",
      "Epoch 128/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.6641e-04 - val_loss: 4.6534e-04\n",
      "Epoch 129/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.6588e-04 - val_loss: 4.6519e-04\n",
      "Epoch 130/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.6531e-04 - val_loss: 4.6460e-04\n",
      "Epoch 131/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.6477e-04 - val_loss: 4.6361e-04\n",
      "Epoch 132/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.6413e-04 - val_loss: 4.6320e-04\n",
      "Epoch 133/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.6361e-04 - val_loss: 4.6251e-04\n",
      "Epoch 134/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.6315e-04 - val_loss: 4.6256e-04\n",
      "Epoch 135/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.6263e-04 - val_loss: 4.6201e-04\n",
      "Epoch 136/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.6200e-04 - val_loss: 4.6159e-04\n",
      "Epoch 137/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.6164e-04 - val_loss: 4.6075e-04\n",
      "Epoch 138/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.6105e-04 - val_loss: 4.6115e-04\n",
      "Epoch 139/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.6053e-04 - val_loss: 4.6032e-04\n",
      "Epoch 140/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.6010e-04 - val_loss: 4.5981e-04\n",
      "Epoch 141/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5949e-04 - val_loss: 4.5991e-04\n",
      "Epoch 142/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5931e-04 - val_loss: 4.5895e-04\n",
      "Epoch 143/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5863e-04 - val_loss: 4.5844e-04\n",
      "Epoch 144/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5820e-04 - val_loss: 4.5767e-04\n",
      "Epoch 145/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5786e-04 - val_loss: 4.5746e-04\n",
      "Epoch 146/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5737e-04 - val_loss: 4.5698e-04\n",
      "Epoch 147/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.5690e-04 - val_loss: 4.5695e-04\n",
      "Epoch 148/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.5637e-04 - val_loss: 4.5622e-04\n",
      "Epoch 149/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5600e-04 - val_loss: 4.5619e-04\n",
      "Epoch 150/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5550e-04 - val_loss: 4.5537e-04\n",
      "Epoch 151/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5515e-04 - val_loss: 4.5544e-04\n",
      "Epoch 152/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5482e-04 - val_loss: 4.5455e-04\n",
      "Epoch 153/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5436e-04 - val_loss: 4.5458e-04\n",
      "Epoch 154/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5387e-04 - val_loss: 4.5397e-04\n",
      "Epoch 155/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5347e-04 - val_loss: 4.5362e-04\n",
      "Epoch 156/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5298e-04 - val_loss: 4.5359e-04\n",
      "Epoch 157/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5272e-04 - val_loss: 4.5305e-04\n",
      "Epoch 158/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5240e-04 - val_loss: 4.5220e-04\n",
      "Epoch 159/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5197e-04 - val_loss: 4.5195e-04\n",
      "Epoch 160/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5150e-04 - val_loss: 4.5147e-04\n",
      "Epoch 161/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5124e-04 - val_loss: 4.5165e-04\n",
      "Epoch 162/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5069e-04 - val_loss: 4.5144e-04\n",
      "Epoch 163/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5039e-04 - val_loss: 4.5076e-04\n",
      "Epoch 164/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.5008e-04 - val_loss: 4.5029e-04\n",
      "Epoch 165/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.4958e-04 - val_loss: 4.4993e-04\n",
      "Epoch 166/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.4938e-04 - val_loss: 4.4957e-04\n",
      "Epoch 167/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.4892e-04 - val_loss: 4.4914e-04\n",
      "Epoch 168/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.4844e-04 - val_loss: 4.4916e-04\n",
      "Epoch 169/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.4801e-04 - val_loss: 4.4843e-04\n",
      "Epoch 170/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.4775e-04 - val_loss: 4.4897e-04\n",
      "Epoch 171/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.4734e-04 - val_loss: 4.4800e-04\n",
      "Epoch 172/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.4707e-04 - val_loss: 4.4756e-04\n",
      "Epoch 173/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.4661e-04 - val_loss: 4.4704e-04\n",
      "Epoch 174/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.4623e-04 - val_loss: 4.4654e-04\n",
      "Epoch 175/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.4597e-04 - val_loss: 4.4668e-04\n",
      "Epoch 176/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.4549e-04 - val_loss: 4.4631e-04\n",
      "Epoch 177/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.4524e-04 - val_loss: 4.4612e-04\n",
      "Epoch 178/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.4476e-04 - val_loss: 4.4584e-04\n",
      "Epoch 179/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.4441e-04 - val_loss: 4.4528e-04\n",
      "Epoch 180/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.4413e-04 - val_loss: 4.4462e-04\n",
      "Epoch 181/1000\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 4.4377e-04 - val_loss: 4.4465e-04\n",
      "Epoch 182/1000\n",
      "250/250 [==============================] - 2s 9ms/step - loss: 4.4339e-04 - val_loss: 4.4443e-04\n",
      "Epoch 183/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.4299e-04 - val_loss: 4.4430e-04\n",
      "Epoch 184/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.4274e-04 - val_loss: 4.4385e-04\n",
      "Epoch 185/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.4235e-04 - val_loss: 4.4320e-04\n",
      "Epoch 186/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.4207e-04 - val_loss: 4.4305e-04\n",
      "Epoch 187/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.4160e-04 - val_loss: 4.4294e-04\n",
      "Epoch 188/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.4136e-04 - val_loss: 4.4192e-04\n",
      "Epoch 189/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.4105e-04 - val_loss: 4.4178e-04\n",
      "Epoch 190/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.4074e-04 - val_loss: 4.4170e-04\n",
      "Epoch 191/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.4031e-04 - val_loss: 4.4132e-04\n",
      "Epoch 192/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.4015e-04 - val_loss: 4.4142e-04\n",
      "Epoch 193/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3985e-04 - val_loss: 4.4065e-04\n",
      "Epoch 194/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3936e-04 - val_loss: 4.4022e-04\n",
      "Epoch 195/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3915e-04 - val_loss: 4.4021e-04\n",
      "Epoch 196/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3877e-04 - val_loss: 4.4000e-04\n",
      "Epoch 197/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3859e-04 - val_loss: 4.3965e-04\n",
      "Epoch 198/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3827e-04 - val_loss: 4.3925e-04\n",
      "Epoch 199/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3794e-04 - val_loss: 4.3960e-04\n",
      "Epoch 200/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3778e-04 - val_loss: 4.3838e-04\n",
      "Epoch 201/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3740e-04 - val_loss: 4.3853e-04\n",
      "Epoch 202/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3697e-04 - val_loss: 4.3805e-04\n",
      "Epoch 203/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3682e-04 - val_loss: 4.3742e-04\n",
      "Epoch 204/1000\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 4.3647e-04 - val_loss: 4.3751e-04\n",
      "Epoch 205/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3615e-04 - val_loss: 4.3869e-04\n",
      "Epoch 206/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3605e-04 - val_loss: 4.3730e-04\n",
      "Epoch 207/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3557e-04 - val_loss: 4.3675e-04\n",
      "Epoch 208/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3543e-04 - val_loss: 4.3647e-04\n",
      "Epoch 209/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3499e-04 - val_loss: 4.3643e-04\n",
      "Epoch 210/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3491e-04 - val_loss: 4.3547e-04\n",
      "Epoch 211/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3439e-04 - val_loss: 4.3629e-04\n",
      "Epoch 212/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3423e-04 - val_loss: 4.3570e-04\n",
      "Epoch 213/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3391e-04 - val_loss: 4.3497e-04\n",
      "Epoch 214/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3369e-04 - val_loss: 4.3497e-04\n",
      "Epoch 215/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3346e-04 - val_loss: 4.3512e-04\n",
      "Epoch 216/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3313e-04 - val_loss: 4.3425e-04\n",
      "Epoch 217/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3287e-04 - val_loss: 4.3417e-04\n",
      "Epoch 218/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3264e-04 - val_loss: 4.3433e-04\n",
      "Epoch 219/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3247e-04 - val_loss: 4.3428e-04\n",
      "Epoch 220/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3223e-04 - val_loss: 4.3341e-04\n",
      "Epoch 221/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.3179e-04 - val_loss: 4.3327e-04\n",
      "Epoch 222/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.3151e-04 - val_loss: 4.3294e-04\n",
      "Epoch 223/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.3134e-04 - val_loss: 4.3259e-04\n",
      "Epoch 224/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.3110e-04 - val_loss: 4.3381e-04\n",
      "Epoch 225/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.3085e-04 - val_loss: 4.3300e-04\n",
      "Epoch 226/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.3046e-04 - val_loss: 4.3226e-04\n",
      "Epoch 227/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.3026e-04 - val_loss: 4.3259e-04\n",
      "Epoch 228/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.3006e-04 - val_loss: 4.3125e-04\n",
      "Epoch 229/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2993e-04 - val_loss: 4.3160e-04\n",
      "Epoch 230/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2946e-04 - val_loss: 4.3078e-04\n",
      "Epoch 231/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2931e-04 - val_loss: 4.3105e-04\n",
      "Epoch 232/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2892e-04 - val_loss: 4.3052e-04\n",
      "Epoch 233/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.2873e-04 - val_loss: 4.3025e-04\n",
      "Epoch 234/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.2847e-04 - val_loss: 4.3125e-04\n",
      "Epoch 235/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2843e-04 - val_loss: 4.2973e-04\n",
      "Epoch 236/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2816e-04 - val_loss: 4.2961e-04\n",
      "Epoch 237/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2782e-04 - val_loss: 4.2953e-04\n",
      "Epoch 238/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2764e-04 - val_loss: 4.2877e-04\n",
      "Epoch 239/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2738e-04 - val_loss: 4.2939e-04\n",
      "Epoch 240/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2714e-04 - val_loss: 4.2961e-04\n",
      "Epoch 241/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2690e-04 - val_loss: 4.2927e-04\n",
      "Epoch 242/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2655e-04 - val_loss: 4.2865e-04\n",
      "Epoch 243/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2648e-04 - val_loss: 4.2937e-04\n",
      "Epoch 244/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2622e-04 - val_loss: 4.2867e-04\n",
      "Epoch 245/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2594e-04 - val_loss: 4.2783e-04\n",
      "Epoch 246/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2595e-04 - val_loss: 4.2869e-04\n",
      "Epoch 247/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2532e-04 - val_loss: 4.2713e-04\n",
      "Epoch 248/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2520e-04 - val_loss: 4.2690e-04\n",
      "Epoch 249/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2512e-04 - val_loss: 4.2659e-04\n",
      "Epoch 250/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2470e-04 - val_loss: 4.2695e-04\n",
      "Epoch 251/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2453e-04 - val_loss: 4.2624e-04\n",
      "Epoch 252/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2437e-04 - val_loss: 4.2593e-04\n",
      "Epoch 253/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2410e-04 - val_loss: 4.2558e-04\n",
      "Epoch 254/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2393e-04 - val_loss: 4.2666e-04\n",
      "Epoch 255/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2366e-04 - val_loss: 4.2508e-04\n",
      "Epoch 256/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2353e-04 - val_loss: 4.2637e-04\n",
      "Epoch 257/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2319e-04 - val_loss: 4.2543e-04\n",
      "Epoch 258/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2302e-04 - val_loss: 4.2481e-04\n",
      "Epoch 259/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2269e-04 - val_loss: 4.2548e-04\n",
      "Epoch 260/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2263e-04 - val_loss: 4.2497e-04\n",
      "Epoch 261/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2227e-04 - val_loss: 4.2426e-04\n",
      "Epoch 262/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2212e-04 - val_loss: 4.2378e-04\n",
      "Epoch 263/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2182e-04 - val_loss: 4.2353e-04\n",
      "Epoch 264/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2162e-04 - val_loss: 4.2335e-04\n",
      "Epoch 265/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2146e-04 - val_loss: 4.2346e-04\n",
      "Epoch 266/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2124e-04 - val_loss: 4.2449e-04\n",
      "Epoch 267/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2101e-04 - val_loss: 4.2246e-04\n",
      "Epoch 268/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2081e-04 - val_loss: 4.2294e-04\n",
      "Epoch 269/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2056e-04 - val_loss: 4.2261e-04\n",
      "Epoch 270/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2044e-04 - val_loss: 4.2211e-04\n",
      "Epoch 271/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2019e-04 - val_loss: 4.2153e-04\n",
      "Epoch 272/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.2005e-04 - val_loss: 4.2163e-04\n",
      "Epoch 273/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1960e-04 - val_loss: 4.2182e-04\n",
      "Epoch 274/1000\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 4.1976e-04 - val_loss: 4.2140e-04\n",
      "Epoch 275/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.1925e-04 - val_loss: 4.2114e-04\n",
      "Epoch 276/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1915e-04 - val_loss: 4.2114e-04\n",
      "Epoch 277/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1893e-04 - val_loss: 4.2076e-04\n",
      "Epoch 278/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1881e-04 - val_loss: 4.2039e-04\n",
      "Epoch 279/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.1835e-04 - val_loss: 4.2057e-04\n",
      "Epoch 280/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1831e-04 - val_loss: 4.2075e-04\n",
      "Epoch 281/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1822e-04 - val_loss: 4.2212e-04\n",
      "Epoch 282/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1796e-04 - val_loss: 4.1973e-04\n",
      "Epoch 283/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1780e-04 - val_loss: 4.1936e-04\n",
      "Epoch 284/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1754e-04 - val_loss: 4.2053e-04\n",
      "Epoch 285/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1731e-04 - val_loss: 4.1960e-04\n",
      "Epoch 286/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1715e-04 - val_loss: 4.1881e-04\n",
      "Epoch 287/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1690e-04 - val_loss: 4.1900e-04\n",
      "Epoch 288/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1674e-04 - val_loss: 4.1940e-04\n",
      "Epoch 289/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1653e-04 - val_loss: 4.1827e-04\n",
      "Epoch 290/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1629e-04 - val_loss: 4.1835e-04\n",
      "Epoch 291/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1610e-04 - val_loss: 4.1776e-04\n",
      "Epoch 292/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1604e-04 - val_loss: 4.1814e-04\n",
      "Epoch 293/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1569e-04 - val_loss: 4.1751e-04\n",
      "Epoch 294/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1555e-04 - val_loss: 4.1771e-04\n",
      "Epoch 295/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1520e-04 - val_loss: 4.1747e-04\n",
      "Epoch 296/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1523e-04 - val_loss: 4.1732e-04\n",
      "Epoch 297/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1499e-04 - val_loss: 4.1709e-04\n",
      "Epoch 298/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1483e-04 - val_loss: 4.1748e-04\n",
      "Epoch 299/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1452e-04 - val_loss: 4.1666e-04\n",
      "Epoch 300/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1448e-04 - val_loss: 4.1642e-04\n",
      "Epoch 301/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1412e-04 - val_loss: 4.1664e-04\n",
      "Epoch 302/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1393e-04 - val_loss: 4.1680e-04\n",
      "Epoch 303/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1387e-04 - val_loss: 4.1645e-04\n",
      "Epoch 304/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1366e-04 - val_loss: 4.1612e-04\n",
      "Epoch 305/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1359e-04 - val_loss: 4.1561e-04\n",
      "Epoch 306/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1346e-04 - val_loss: 4.1550e-04\n",
      "Epoch 307/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1329e-04 - val_loss: 4.1591e-04\n",
      "Epoch 308/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1298e-04 - val_loss: 4.1513e-04\n",
      "Epoch 309/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1292e-04 - val_loss: 4.1550e-04\n",
      "Epoch 310/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1255e-04 - val_loss: 4.1486e-04\n",
      "Epoch 311/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1244e-04 - val_loss: 4.1478e-04\n",
      "Epoch 312/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1212e-04 - val_loss: 4.1498e-04\n",
      "Epoch 313/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1206e-04 - val_loss: 4.1454e-04\n",
      "Epoch 314/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1189e-04 - val_loss: 4.1439e-04\n",
      "Epoch 315/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1176e-04 - val_loss: 4.1391e-04\n",
      "Epoch 316/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1135e-04 - val_loss: 4.1371e-04\n",
      "Epoch 317/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1136e-04 - val_loss: 4.1375e-04\n",
      "Epoch 318/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1106e-04 - val_loss: 4.1348e-04\n",
      "Epoch 319/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1108e-04 - val_loss: 4.1350e-04\n",
      "Epoch 320/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1076e-04 - val_loss: 4.1359e-04\n",
      "Epoch 321/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1073e-04 - val_loss: 4.1298e-04\n",
      "Epoch 322/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.1041e-04 - val_loss: 4.1284e-04\n",
      "Epoch 323/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.1030e-04 - val_loss: 4.1256e-04\n",
      "Epoch 324/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.1014e-04 - val_loss: 4.1229e-04\n",
      "Epoch 325/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0984e-04 - val_loss: 4.1212e-04\n",
      "Epoch 326/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0983e-04 - val_loss: 4.1246e-04\n",
      "Epoch 327/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0939e-04 - val_loss: 4.1194e-04\n",
      "Epoch 328/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0930e-04 - val_loss: 4.1198e-04\n",
      "Epoch 329/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0922e-04 - val_loss: 4.1130e-04\n",
      "Epoch 330/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0897e-04 - val_loss: 4.1129e-04\n",
      "Epoch 331/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0879e-04 - val_loss: 4.1166e-04\n",
      "Epoch 332/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0865e-04 - val_loss: 4.1125e-04\n",
      "Epoch 333/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0852e-04 - val_loss: 4.1060e-04\n",
      "Epoch 334/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0822e-04 - val_loss: 4.1092e-04\n",
      "Epoch 335/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0817e-04 - val_loss: 4.1125e-04\n",
      "Epoch 336/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0796e-04 - val_loss: 4.1092e-04\n",
      "Epoch 337/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0765e-04 - val_loss: 4.1063e-04\n",
      "Epoch 338/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0758e-04 - val_loss: 4.1014e-04\n",
      "Epoch 339/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0737e-04 - val_loss: 4.0996e-04\n",
      "Epoch 340/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0713e-04 - val_loss: 4.1014e-04\n",
      "Epoch 341/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0708e-04 - val_loss: 4.0985e-04\n",
      "Epoch 342/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0697e-04 - val_loss: 4.0949e-04\n",
      "Epoch 343/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0666e-04 - val_loss: 4.0998e-04\n",
      "Epoch 344/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0649e-04 - val_loss: 4.0989e-04\n",
      "Epoch 345/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0641e-04 - val_loss: 4.0848e-04\n",
      "Epoch 346/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0586e-04 - val_loss: 4.0867e-04\n",
      "Epoch 347/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0597e-04 - val_loss: 4.0842e-04\n",
      "Epoch 348/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0577e-04 - val_loss: 4.0864e-04\n",
      "Epoch 349/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0570e-04 - val_loss: 4.0834e-04\n",
      "Epoch 350/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0529e-04 - val_loss: 4.0808e-04\n",
      "Epoch 351/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0533e-04 - val_loss: 4.0803e-04\n",
      "Epoch 352/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0522e-04 - val_loss: 4.0967e-04\n",
      "Epoch 353/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0488e-04 - val_loss: 4.0811e-04\n",
      "Epoch 354/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0477e-04 - val_loss: 4.0780e-04\n",
      "Epoch 355/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0462e-04 - val_loss: 4.0766e-04\n",
      "Epoch 356/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0434e-04 - val_loss: 4.0831e-04\n",
      "Epoch 357/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0412e-04 - val_loss: 4.0758e-04\n",
      "Epoch 358/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0389e-04 - val_loss: 4.0667e-04\n",
      "Epoch 359/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0372e-04 - val_loss: 4.0681e-04\n",
      "Epoch 360/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 4.0390e-04 - val_loss: 4.0673e-04\n",
      "Epoch 361/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0326e-04 - val_loss: 4.0692e-04\n",
      "Epoch 362/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0325e-04 - val_loss: 4.0610e-04\n",
      "Epoch 363/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0316e-04 - val_loss: 4.0617e-04\n",
      "Epoch 364/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0278e-04 - val_loss: 4.0603e-04\n",
      "Epoch 365/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0268e-04 - val_loss: 4.0638e-04\n",
      "Epoch 366/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0243e-04 - val_loss: 4.0592e-04\n",
      "Epoch 367/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0237e-04 - val_loss: 4.0562e-04\n",
      "Epoch 368/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0240e-04 - val_loss: 4.0565e-04\n",
      "Epoch 369/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0203e-04 - val_loss: 4.0484e-04\n",
      "Epoch 370/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0197e-04 - val_loss: 4.0504e-04\n",
      "Epoch 371/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0178e-04 - val_loss: 4.0498e-04\n",
      "Epoch 372/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0139e-04 - val_loss: 4.0459e-04\n",
      "Epoch 373/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0142e-04 - val_loss: 4.0508e-04\n",
      "Epoch 374/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0131e-04 - val_loss: 4.0431e-04\n",
      "Epoch 375/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0105e-04 - val_loss: 4.0377e-04\n",
      "Epoch 376/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0076e-04 - val_loss: 4.0432e-04\n",
      "Epoch 377/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0052e-04 - val_loss: 4.0367e-04\n",
      "Epoch 378/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0040e-04 - val_loss: 4.0383e-04\n",
      "Epoch 379/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0014e-04 - val_loss: 4.0375e-04\n",
      "Epoch 380/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 4.0015e-04 - val_loss: 4.0329e-04\n",
      "Epoch 381/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9997e-04 - val_loss: 4.0336e-04\n",
      "Epoch 382/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9952e-04 - val_loss: 4.0341e-04\n",
      "Epoch 383/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9965e-04 - val_loss: 4.0304e-04\n",
      "Epoch 384/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9926e-04 - val_loss: 4.0280e-04\n",
      "Epoch 385/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9919e-04 - val_loss: 4.0258e-04\n",
      "Epoch 386/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9901e-04 - val_loss: 4.0237e-04\n",
      "Epoch 387/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9877e-04 - val_loss: 4.0280e-04\n",
      "Epoch 388/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9852e-04 - val_loss: 4.0218e-04\n",
      "Epoch 389/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9842e-04 - val_loss: 4.0191e-04\n",
      "Epoch 390/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9824e-04 - val_loss: 4.0163e-04\n",
      "Epoch 391/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9820e-04 - val_loss: 4.0145e-04\n",
      "Epoch 392/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9780e-04 - val_loss: 4.0226e-04\n",
      "Epoch 393/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9773e-04 - val_loss: 4.0172e-04\n",
      "Epoch 394/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9742e-04 - val_loss: 4.0178e-04\n",
      "Epoch 395/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9746e-04 - val_loss: 4.0146e-04\n",
      "Epoch 396/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9721e-04 - val_loss: 4.0109e-04\n",
      "Epoch 397/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9703e-04 - val_loss: 4.0059e-04\n",
      "Epoch 398/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.9680e-04 - val_loss: 4.0082e-04\n",
      "Epoch 399/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9648e-04 - val_loss: 4.0150e-04\n",
      "Epoch 400/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9653e-04 - val_loss: 4.0048e-04\n",
      "Epoch 401/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9617e-04 - val_loss: 3.9982e-04\n",
      "Epoch 402/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9619e-04 - val_loss: 4.0020e-04\n",
      "Epoch 403/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9601e-04 - val_loss: 3.9968e-04\n",
      "Epoch 404/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9581e-04 - val_loss: 3.9986e-04\n",
      "Epoch 405/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9552e-04 - val_loss: 3.9953e-04\n",
      "Epoch 406/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9542e-04 - val_loss: 3.9929e-04\n",
      "Epoch 407/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9526e-04 - val_loss: 3.9896e-04\n",
      "Epoch 408/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9512e-04 - val_loss: 3.9965e-04\n",
      "Epoch 409/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9494e-04 - val_loss: 3.9915e-04\n",
      "Epoch 410/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9482e-04 - val_loss: 3.9877e-04\n",
      "Epoch 411/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9464e-04 - val_loss: 3.9899e-04\n",
      "Epoch 412/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9468e-04 - val_loss: 3.9854e-04\n",
      "Epoch 413/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9436e-04 - val_loss: 3.9827e-04\n",
      "Epoch 414/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9421e-04 - val_loss: 3.9832e-04\n",
      "Epoch 415/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.9385e-04 - val_loss: 3.9777e-04\n",
      "Epoch 416/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9373e-04 - val_loss: 3.9799e-04\n",
      "Epoch 417/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9374e-04 - val_loss: 3.9782e-04\n",
      "Epoch 418/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9352e-04 - val_loss: 3.9748e-04\n",
      "Epoch 419/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9344e-04 - val_loss: 3.9750e-04\n",
      "Epoch 420/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9339e-04 - val_loss: 3.9739e-04\n",
      "Epoch 421/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9310e-04 - val_loss: 3.9724e-04\n",
      "Epoch 422/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9290e-04 - val_loss: 3.9710e-04\n",
      "Epoch 423/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9255e-04 - val_loss: 3.9732e-04\n",
      "Epoch 424/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9273e-04 - val_loss: 3.9703e-04\n",
      "Epoch 425/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9259e-04 - val_loss: 3.9685e-04\n",
      "Epoch 426/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9236e-04 - val_loss: 3.9667e-04\n",
      "Epoch 427/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9228e-04 - val_loss: 3.9705e-04\n",
      "Epoch 428/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.9198e-04 - val_loss: 3.9686e-04\n",
      "Epoch 429/1000\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 3.9212e-04 - val_loss: 3.9726e-04\n",
      "Epoch 430/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9175e-04 - val_loss: 3.9588e-04\n",
      "Epoch 431/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9174e-04 - val_loss: 3.9614e-04\n",
      "Epoch 432/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9150e-04 - val_loss: 3.9567e-04\n",
      "Epoch 433/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9134e-04 - val_loss: 3.9535e-04\n",
      "Epoch 434/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9131e-04 - val_loss: 3.9541e-04\n",
      "Epoch 435/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.9127e-04 - val_loss: 3.9576e-04\n",
      "Epoch 436/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9077e-04 - val_loss: 3.9561e-04\n",
      "Epoch 437/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9102e-04 - val_loss: 3.9551e-04\n",
      "Epoch 438/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9078e-04 - val_loss: 3.9671e-04\n",
      "Epoch 439/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9058e-04 - val_loss: 3.9586e-04\n",
      "Epoch 440/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9028e-04 - val_loss: 3.9535e-04\n",
      "Epoch 441/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9032e-04 - val_loss: 3.9476e-04\n",
      "Epoch 442/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.9019e-04 - val_loss: 3.9474e-04\n",
      "Epoch 443/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8989e-04 - val_loss: 3.9458e-04\n",
      "Epoch 444/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8981e-04 - val_loss: 3.9434e-04\n",
      "Epoch 445/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8971e-04 - val_loss: 3.9412e-04\n",
      "Epoch 446/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8950e-04 - val_loss: 3.9433e-04\n",
      "Epoch 447/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8966e-04 - val_loss: 3.9345e-04\n",
      "Epoch 448/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8931e-04 - val_loss: 3.9429e-04\n",
      "Epoch 449/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8928e-04 - val_loss: 3.9317e-04\n",
      "Epoch 450/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8908e-04 - val_loss: 3.9397e-04\n",
      "Epoch 451/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8915e-04 - val_loss: 3.9373e-04\n",
      "Epoch 452/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8902e-04 - val_loss: 3.9404e-04\n",
      "Epoch 453/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8882e-04 - val_loss: 3.9361e-04\n",
      "Epoch 454/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8860e-04 - val_loss: 3.9330e-04\n",
      "Epoch 455/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8859e-04 - val_loss: 3.9378e-04\n",
      "Epoch 456/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8831e-04 - val_loss: 3.9292e-04\n",
      "Epoch 457/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8823e-04 - val_loss: 3.9308e-04\n",
      "Epoch 458/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8810e-04 - val_loss: 3.9363e-04\n",
      "Epoch 459/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8787e-04 - val_loss: 3.9295e-04\n",
      "Epoch 460/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8784e-04 - val_loss: 3.9220e-04\n",
      "Epoch 461/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8767e-04 - val_loss: 3.9263e-04\n",
      "Epoch 462/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8761e-04 - val_loss: 3.9265e-04\n",
      "Epoch 463/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8750e-04 - val_loss: 3.9228e-04\n",
      "Epoch 464/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8754e-04 - val_loss: 3.9218e-04\n",
      "Epoch 465/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8716e-04 - val_loss: 3.9218e-04\n",
      "Epoch 466/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8697e-04 - val_loss: 3.9159e-04\n",
      "Epoch 467/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8716e-04 - val_loss: 3.9241e-04\n",
      "Epoch 468/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8716e-04 - val_loss: 3.9182e-04\n",
      "Epoch 469/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8672e-04 - val_loss: 3.9126e-04\n",
      "Epoch 470/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8670e-04 - val_loss: 3.9128e-04\n",
      "Epoch 471/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8654e-04 - val_loss: 3.9121e-04\n",
      "Epoch 472/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8646e-04 - val_loss: 3.9180e-04\n",
      "Epoch 473/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8634e-04 - val_loss: 3.9085e-04\n",
      "Epoch 474/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8634e-04 - val_loss: 3.9116e-04\n",
      "Epoch 475/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8610e-04 - val_loss: 3.9109e-04\n",
      "Epoch 476/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8610e-04 - val_loss: 3.9052e-04\n",
      "Epoch 477/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8572e-04 - val_loss: 3.9045e-04\n",
      "Epoch 478/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8577e-04 - val_loss: 3.9046e-04\n",
      "Epoch 479/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8578e-04 - val_loss: 3.9056e-04\n",
      "Epoch 480/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8568e-04 - val_loss: 3.9045e-04\n",
      "Epoch 481/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8536e-04 - val_loss: 3.9030e-04\n",
      "Epoch 482/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8530e-04 - val_loss: 3.8997e-04\n",
      "Epoch 483/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8548e-04 - val_loss: 3.8993e-04\n",
      "Epoch 484/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8527e-04 - val_loss: 3.8982e-04\n",
      "Epoch 485/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8502e-04 - val_loss: 3.9069e-04\n",
      "Epoch 486/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8495e-04 - val_loss: 3.8949e-04\n",
      "Epoch 487/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8475e-04 - val_loss: 3.8942e-04\n",
      "Epoch 488/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8473e-04 - val_loss: 3.8922e-04\n",
      "Epoch 489/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8462e-04 - val_loss: 3.8950e-04\n",
      "Epoch 490/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8463e-04 - val_loss: 3.8886e-04\n",
      "Epoch 491/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8444e-04 - val_loss: 3.8933e-04\n",
      "Epoch 492/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8425e-04 - val_loss: 3.8904e-04\n",
      "Epoch 493/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8427e-04 - val_loss: 3.8927e-04\n",
      "Epoch 494/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8422e-04 - val_loss: 3.8920e-04\n",
      "Epoch 495/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8406e-04 - val_loss: 3.8844e-04\n",
      "Epoch 496/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8400e-04 - val_loss: 3.8934e-04\n",
      "Epoch 497/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8385e-04 - val_loss: 3.8887e-04\n",
      "Epoch 498/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8376e-04 - val_loss: 3.8840e-04\n",
      "Epoch 499/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8361e-04 - val_loss: 3.8871e-04\n",
      "Epoch 500/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8348e-04 - val_loss: 3.8897e-04\n",
      "Epoch 501/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8340e-04 - val_loss: 3.8829e-04\n",
      "Epoch 502/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8335e-04 - val_loss: 3.8843e-04\n",
      "Epoch 503/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8321e-04 - val_loss: 3.8934e-04\n",
      "Epoch 504/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.8322e-04 - val_loss: 3.8857e-04\n",
      "Epoch 505/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.8293e-04 - val_loss: 3.8808e-04\n",
      "Epoch 506/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8291e-04 - val_loss: 3.8737e-04\n",
      "Epoch 507/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8277e-04 - val_loss: 3.8780e-04\n",
      "Epoch 508/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8296e-04 - val_loss: 3.8909e-04\n",
      "Epoch 509/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8286e-04 - val_loss: 3.8837e-04\n",
      "Epoch 510/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8239e-04 - val_loss: 3.8755e-04\n",
      "Epoch 511/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8242e-04 - val_loss: 3.8724e-04\n",
      "Epoch 512/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8255e-04 - val_loss: 3.8732e-04\n",
      "Epoch 513/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8224e-04 - val_loss: 3.8684e-04\n",
      "Epoch 514/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8198e-04 - val_loss: 3.8756e-04\n",
      "Epoch 515/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8203e-04 - val_loss: 3.8798e-04\n",
      "Epoch 516/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8191e-04 - val_loss: 3.8649e-04\n",
      "Epoch 517/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8171e-04 - val_loss: 3.8639e-04\n",
      "Epoch 518/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8167e-04 - val_loss: 3.8689e-04\n",
      "Epoch 519/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8170e-04 - val_loss: 3.8636e-04\n",
      "Epoch 520/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8148e-04 - val_loss: 3.8649e-04\n",
      "Epoch 521/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8150e-04 - val_loss: 3.8601e-04\n",
      "Epoch 522/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8151e-04 - val_loss: 3.8638e-04\n",
      "Epoch 523/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8133e-04 - val_loss: 3.8623e-04\n",
      "Epoch 524/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8121e-04 - val_loss: 3.8640e-04\n",
      "Epoch 525/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8121e-04 - val_loss: 3.8626e-04\n",
      "Epoch 526/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8110e-04 - val_loss: 3.8658e-04\n",
      "Epoch 527/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8094e-04 - val_loss: 3.8580e-04\n",
      "Epoch 528/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8074e-04 - val_loss: 3.8550e-04\n",
      "Epoch 529/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8078e-04 - val_loss: 3.8533e-04\n",
      "Epoch 530/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8052e-04 - val_loss: 3.8539e-04\n",
      "Epoch 531/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8061e-04 - val_loss: 3.8566e-04\n",
      "Epoch 532/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8063e-04 - val_loss: 3.8581e-04\n",
      "Epoch 533/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8037e-04 - val_loss: 3.8544e-04\n",
      "Epoch 534/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8022e-04 - val_loss: 3.8498e-04\n",
      "Epoch 535/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8007e-04 - val_loss: 3.8486e-04\n",
      "Epoch 536/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8031e-04 - val_loss: 3.8611e-04\n",
      "Epoch 537/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7995e-04 - val_loss: 3.8489e-04\n",
      "Epoch 538/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.8007e-04 - val_loss: 3.8539e-04\n",
      "Epoch 539/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7998e-04 - val_loss: 3.8470e-04\n",
      "Epoch 540/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7985e-04 - val_loss: 3.8497e-04\n",
      "Epoch 541/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7968e-04 - val_loss: 3.8499e-04\n",
      "Epoch 542/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7967e-04 - val_loss: 3.8521e-04\n",
      "Epoch 543/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7942e-04 - val_loss: 3.8461e-04\n",
      "Epoch 544/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7954e-04 - val_loss: 3.8514e-04\n",
      "Epoch 545/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7938e-04 - val_loss: 3.8456e-04\n",
      "Epoch 546/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7925e-04 - val_loss: 3.8475e-04\n",
      "Epoch 547/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7913e-04 - val_loss: 3.8420e-04\n",
      "Epoch 548/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7915e-04 - val_loss: 3.8393e-04\n",
      "Epoch 549/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7903e-04 - val_loss: 3.8375e-04\n",
      "Epoch 550/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7915e-04 - val_loss: 3.8426e-04\n",
      "Epoch 551/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7878e-04 - val_loss: 3.8403e-04\n",
      "Epoch 552/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7867e-04 - val_loss: 3.8345e-04\n",
      "Epoch 553/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7884e-04 - val_loss: 3.8442e-04\n",
      "Epoch 554/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7861e-04 - val_loss: 3.8492e-04\n",
      "Epoch 555/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7835e-04 - val_loss: 3.8413e-04\n",
      "Epoch 556/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7846e-04 - val_loss: 3.8396e-04\n",
      "Epoch 557/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7841e-04 - val_loss: 3.8326e-04\n",
      "Epoch 558/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7823e-04 - val_loss: 3.8347e-04\n",
      "Epoch 559/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7828e-04 - val_loss: 3.8312e-04\n",
      "Epoch 560/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7806e-04 - val_loss: 3.8290e-04\n",
      "Epoch 561/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7815e-04 - val_loss: 3.8307e-04\n",
      "Epoch 562/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7793e-04 - val_loss: 3.8337e-04\n",
      "Epoch 563/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7785e-04 - val_loss: 3.8351e-04\n",
      "Epoch 564/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7783e-04 - val_loss: 3.8318e-04\n",
      "Epoch 565/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7782e-04 - val_loss: 3.8317e-04\n",
      "Epoch 566/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7774e-04 - val_loss: 3.8306e-04\n",
      "Epoch 567/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7745e-04 - val_loss: 3.8271e-04\n",
      "Epoch 568/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7741e-04 - val_loss: 3.8240e-04\n",
      "Epoch 569/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7739e-04 - val_loss: 3.8234e-04\n",
      "Epoch 570/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7722e-04 - val_loss: 3.8268e-04\n",
      "Epoch 571/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7727e-04 - val_loss: 3.8230e-04\n",
      "Epoch 572/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7720e-04 - val_loss: 3.8263e-04\n",
      "Epoch 573/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7692e-04 - val_loss: 3.8214e-04\n",
      "Epoch 574/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7690e-04 - val_loss: 3.8199e-04\n",
      "Epoch 575/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7693e-04 - val_loss: 3.8210e-04\n",
      "Epoch 576/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7692e-04 - val_loss: 3.8172e-04\n",
      "Epoch 577/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7674e-04 - val_loss: 3.8182e-04\n",
      "Epoch 578/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7657e-04 - val_loss: 3.8196e-04\n",
      "Epoch 579/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7665e-04 - val_loss: 3.8183e-04\n",
      "Epoch 580/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7641e-04 - val_loss: 3.8170e-04\n",
      "Epoch 581/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7630e-04 - val_loss: 3.8241e-04\n",
      "Epoch 582/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7637e-04 - val_loss: 3.8134e-04\n",
      "Epoch 583/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7641e-04 - val_loss: 3.8105e-04\n",
      "Epoch 584/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7606e-04 - val_loss: 3.8199e-04\n",
      "Epoch 585/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7620e-04 - val_loss: 3.8124e-04\n",
      "Epoch 586/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7591e-04 - val_loss: 3.8196e-04\n",
      "Epoch 587/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7602e-04 - val_loss: 3.8128e-04\n",
      "Epoch 588/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7606e-04 - val_loss: 3.8181e-04\n",
      "Epoch 589/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7583e-04 - val_loss: 3.8250e-04\n",
      "Epoch 590/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7578e-04 - val_loss: 3.8113e-04\n",
      "Epoch 591/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7562e-04 - val_loss: 3.8138e-04\n",
      "Epoch 592/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7546e-04 - val_loss: 3.8107e-04\n",
      "Epoch 593/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7575e-04 - val_loss: 3.8064e-04\n",
      "Epoch 594/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.7538e-04 - val_loss: 3.8080e-04\n",
      "Epoch 595/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7530e-04 - val_loss: 3.8182e-04\n",
      "Epoch 596/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7534e-04 - val_loss: 3.8112e-04\n",
      "Epoch 597/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7516e-04 - val_loss: 3.8069e-04\n",
      "Epoch 598/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7538e-04 - val_loss: 3.8287e-04\n",
      "Epoch 599/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7508e-04 - val_loss: 3.8052e-04\n",
      "Epoch 600/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7498e-04 - val_loss: 3.8077e-04\n",
      "Epoch 601/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7494e-04 - val_loss: 3.8087e-04\n",
      "Epoch 602/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7475e-04 - val_loss: 3.8019e-04\n",
      "Epoch 603/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7472e-04 - val_loss: 3.8021e-04\n",
      "Epoch 604/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7476e-04 - val_loss: 3.8165e-04\n",
      "Epoch 605/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7456e-04 - val_loss: 3.8037e-04\n",
      "Epoch 606/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7454e-04 - val_loss: 3.7990e-04\n",
      "Epoch 607/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7442e-04 - val_loss: 3.8088e-04\n",
      "Epoch 608/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7437e-04 - val_loss: 3.7936e-04\n",
      "Epoch 609/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7421e-04 - val_loss: 3.8038e-04\n",
      "Epoch 610/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7444e-04 - val_loss: 3.8033e-04\n",
      "Epoch 611/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7430e-04 - val_loss: 3.8065e-04\n",
      "Epoch 612/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7430e-04 - val_loss: 3.7953e-04\n",
      "Epoch 613/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7400e-04 - val_loss: 3.7954e-04\n",
      "Epoch 614/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7398e-04 - val_loss: 3.7924e-04\n",
      "Epoch 615/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7384e-04 - val_loss: 3.8072e-04\n",
      "Epoch 616/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7380e-04 - val_loss: 3.7885e-04\n",
      "Epoch 617/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7370e-04 - val_loss: 3.7944e-04\n",
      "Epoch 618/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7368e-04 - val_loss: 3.7904e-04\n",
      "Epoch 619/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7357e-04 - val_loss: 3.7923e-04\n",
      "Epoch 620/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7371e-04 - val_loss: 3.7888e-04\n",
      "Epoch 621/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7339e-04 - val_loss: 3.7878e-04\n",
      "Epoch 622/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7329e-04 - val_loss: 3.7984e-04\n",
      "Epoch 623/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7319e-04 - val_loss: 3.7913e-04\n",
      "Epoch 624/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7326e-04 - val_loss: 3.7847e-04\n",
      "Epoch 625/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7329e-04 - val_loss: 3.7969e-04\n",
      "Epoch 626/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7305e-04 - val_loss: 3.7864e-04\n",
      "Epoch 627/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7290e-04 - val_loss: 3.7854e-04\n",
      "Epoch 628/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7281e-04 - val_loss: 3.7879e-04\n",
      "Epoch 629/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7284e-04 - val_loss: 3.7857e-04\n",
      "Epoch 630/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7286e-04 - val_loss: 3.7843e-04\n",
      "Epoch 631/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7268e-04 - val_loss: 3.7878e-04\n",
      "Epoch 632/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7270e-04 - val_loss: 3.7882e-04\n",
      "Epoch 633/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7283e-04 - val_loss: 3.7817e-04\n",
      "Epoch 634/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7241e-04 - val_loss: 3.7885e-04\n",
      "Epoch 635/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7292e-04 - val_loss: 3.7822e-04\n",
      "Epoch 636/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7257e-04 - val_loss: 3.7903e-04\n",
      "Epoch 637/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7235e-04 - val_loss: 3.7820e-04\n",
      "Epoch 638/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7213e-04 - val_loss: 3.7823e-04\n",
      "Epoch 639/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7235e-04 - val_loss: 3.7772e-04\n",
      "Epoch 640/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7207e-04 - val_loss: 3.7768e-04\n",
      "Epoch 641/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7208e-04 - val_loss: 3.7981e-04\n",
      "Epoch 642/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7191e-04 - val_loss: 3.7840e-04\n",
      "Epoch 643/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7207e-04 - val_loss: 3.7737e-04\n",
      "Epoch 644/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7182e-04 - val_loss: 3.7750e-04\n",
      "Epoch 645/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7198e-04 - val_loss: 3.7750e-04\n",
      "Epoch 646/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7168e-04 - val_loss: 3.7796e-04\n",
      "Epoch 647/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7157e-04 - val_loss: 3.7749e-04\n",
      "Epoch 648/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7157e-04 - val_loss: 3.7746e-04\n",
      "Epoch 649/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7162e-04 - val_loss: 3.7737e-04\n",
      "Epoch 650/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7155e-04 - val_loss: 3.7715e-04\n",
      "Epoch 651/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7152e-04 - val_loss: 3.7732e-04\n",
      "Epoch 652/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7149e-04 - val_loss: 3.7678e-04\n",
      "Epoch 653/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7119e-04 - val_loss: 3.7648e-04\n",
      "Epoch 654/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7112e-04 - val_loss: 3.7748e-04\n",
      "Epoch 655/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7123e-04 - val_loss: 3.7688e-04\n",
      "Epoch 656/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7100e-04 - val_loss: 3.7673e-04\n",
      "Epoch 657/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7118e-04 - val_loss: 3.7691e-04\n",
      "Epoch 658/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7094e-04 - val_loss: 3.7712e-04\n",
      "Epoch 659/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7088e-04 - val_loss: 3.7683e-04\n",
      "Epoch 660/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7080e-04 - val_loss: 3.7667e-04\n",
      "Epoch 661/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7070e-04 - val_loss: 3.7665e-04\n",
      "Epoch 662/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7066e-04 - val_loss: 3.7741e-04\n",
      "Epoch 663/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7059e-04 - val_loss: 3.7589e-04\n",
      "Epoch 664/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7055e-04 - val_loss: 3.7650e-04\n",
      "Epoch 665/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7064e-04 - val_loss: 3.7633e-04\n",
      "Epoch 666/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7060e-04 - val_loss: 3.7629e-04\n",
      "Epoch 667/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7020e-04 - val_loss: 3.7611e-04\n",
      "Epoch 668/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7039e-04 - val_loss: 3.7726e-04\n",
      "Epoch 669/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7017e-04 - val_loss: 3.7594e-04\n",
      "Epoch 670/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7013e-04 - val_loss: 3.7617e-04\n",
      "Epoch 671/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6991e-04 - val_loss: 3.7638e-04\n",
      "Epoch 672/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7008e-04 - val_loss: 3.7602e-04\n",
      "Epoch 673/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.7000e-04 - val_loss: 3.7576e-04\n",
      "Epoch 674/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6980e-04 - val_loss: 3.7699e-04\n",
      "Epoch 675/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6999e-04 - val_loss: 3.7545e-04\n",
      "Epoch 676/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6993e-04 - val_loss: 3.7601e-04\n",
      "Epoch 677/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6978e-04 - val_loss: 3.7652e-04\n",
      "Epoch 678/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6973e-04 - val_loss: 3.7583e-04\n",
      "Epoch 679/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6929e-04 - val_loss: 3.7559e-04\n",
      "Epoch 680/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6937e-04 - val_loss: 3.7573e-04\n",
      "Epoch 681/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6960e-04 - val_loss: 3.7559e-04\n",
      "Epoch 682/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6973e-04 - val_loss: 3.7561e-04\n",
      "Epoch 683/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6941e-04 - val_loss: 3.7509e-04\n",
      "Epoch 684/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6905e-04 - val_loss: 3.7490e-04\n",
      "Epoch 685/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.6934e-04 - val_loss: 3.7622e-04\n",
      "Epoch 686/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6965e-04 - val_loss: 3.7647e-04\n",
      "Epoch 687/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6906e-04 - val_loss: 3.7508e-04\n",
      "Epoch 688/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6916e-04 - val_loss: 3.7567e-04\n",
      "Epoch 689/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6899e-04 - val_loss: 3.7519e-04\n",
      "Epoch 690/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6894e-04 - val_loss: 3.7493e-04\n",
      "Epoch 691/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6878e-04 - val_loss: 3.7427e-04\n",
      "Epoch 692/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6885e-04 - val_loss: 3.7420e-04\n",
      "Epoch 693/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6884e-04 - val_loss: 3.7457e-04\n",
      "Epoch 694/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6850e-04 - val_loss: 3.7481e-04\n",
      "Epoch 695/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6853e-04 - val_loss: 3.7520e-04\n",
      "Epoch 696/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6848e-04 - val_loss: 3.7481e-04\n",
      "Epoch 697/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6846e-04 - val_loss: 3.7429e-04\n",
      "Epoch 698/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6833e-04 - val_loss: 3.7442e-04\n",
      "Epoch 699/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6838e-04 - val_loss: 3.7437e-04\n",
      "Epoch 700/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6817e-04 - val_loss: 3.7471e-04\n",
      "Epoch 701/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6872e-04 - val_loss: 3.7451e-04\n",
      "Epoch 702/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6818e-04 - val_loss: 3.7392e-04\n",
      "Epoch 703/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6806e-04 - val_loss: 3.7446e-04\n",
      "Epoch 704/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6796e-04 - val_loss: 3.7409e-04\n",
      "Epoch 705/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6795e-04 - val_loss: 3.7411e-04\n",
      "Epoch 706/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6817e-04 - val_loss: 3.7371e-04\n",
      "Epoch 707/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6767e-04 - val_loss: 3.7390e-04\n",
      "Epoch 708/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6776e-04 - val_loss: 3.7377e-04\n",
      "Epoch 709/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6796e-04 - val_loss: 3.7379e-04\n",
      "Epoch 710/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6779e-04 - val_loss: 3.7360e-04\n",
      "Epoch 711/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6757e-04 - val_loss: 3.7348e-04\n",
      "Epoch 712/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6740e-04 - val_loss: 3.7348e-04\n",
      "Epoch 713/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6738e-04 - val_loss: 3.7522e-04\n",
      "Epoch 714/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6750e-04 - val_loss: 3.7475e-04\n",
      "Epoch 715/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6724e-04 - val_loss: 3.7404e-04\n",
      "Epoch 716/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6723e-04 - val_loss: 3.7426e-04\n",
      "Epoch 717/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6712e-04 - val_loss: 3.7420e-04\n",
      "Epoch 718/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6715e-04 - val_loss: 3.7307e-04\n",
      "Epoch 719/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6715e-04 - val_loss: 3.7353e-04\n",
      "Epoch 720/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6685e-04 - val_loss: 3.7437e-04\n",
      "Epoch 721/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6686e-04 - val_loss: 3.7331e-04\n",
      "Epoch 722/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6684e-04 - val_loss: 3.7300e-04\n",
      "Epoch 723/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6675e-04 - val_loss: 3.7297e-04\n",
      "Epoch 724/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6703e-04 - val_loss: 3.7334e-04\n",
      "Epoch 725/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6673e-04 - val_loss: 3.7320e-04\n",
      "Epoch 726/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6669e-04 - val_loss: 3.7294e-04\n",
      "Epoch 727/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6634e-04 - val_loss: 3.7318e-04\n",
      "Epoch 728/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6650e-04 - val_loss: 3.7279e-04\n",
      "Epoch 729/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6638e-04 - val_loss: 3.7268e-04\n",
      "Epoch 730/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6637e-04 - val_loss: 3.7314e-04\n",
      "Epoch 731/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6673e-04 - val_loss: 3.7268e-04\n",
      "Epoch 732/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6618e-04 - val_loss: 3.7339e-04\n",
      "Epoch 733/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6625e-04 - val_loss: 3.7257e-04\n",
      "Epoch 734/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6618e-04 - val_loss: 3.7240e-04\n",
      "Epoch 735/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6626e-04 - val_loss: 3.7226e-04\n",
      "Epoch 736/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6595e-04 - val_loss: 3.7203e-04\n",
      "Epoch 737/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6596e-04 - val_loss: 3.7307e-04\n",
      "Epoch 738/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6580e-04 - val_loss: 3.7224e-04\n",
      "Epoch 739/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6594e-04 - val_loss: 3.7209e-04\n",
      "Epoch 740/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6576e-04 - val_loss: 3.7220e-04\n",
      "Epoch 741/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6580e-04 - val_loss: 3.7199e-04\n",
      "Epoch 742/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6578e-04 - val_loss: 3.7455e-04\n",
      "Epoch 743/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6574e-04 - val_loss: 3.7191e-04\n",
      "Epoch 744/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6553e-04 - val_loss: 3.7151e-04\n",
      "Epoch 745/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6538e-04 - val_loss: 3.7134e-04\n",
      "Epoch 746/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6546e-04 - val_loss: 3.7192e-04\n",
      "Epoch 747/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6539e-04 - val_loss: 3.7183e-04\n",
      "Epoch 748/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6525e-04 - val_loss: 3.7195e-04\n",
      "Epoch 749/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6513e-04 - val_loss: 3.7091e-04\n",
      "Epoch 750/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6518e-04 - val_loss: 3.7083e-04\n",
      "Epoch 751/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6488e-04 - val_loss: 3.7238e-04\n",
      "Epoch 752/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6496e-04 - val_loss: 3.7106e-04\n",
      "Epoch 753/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6483e-04 - val_loss: 3.7145e-04\n",
      "Epoch 754/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6516e-04 - val_loss: 3.7133e-04\n",
      "Epoch 755/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6497e-04 - val_loss: 3.7130e-04\n",
      "Epoch 756/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6492e-04 - val_loss: 3.7149e-04\n",
      "Epoch 757/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6514e-04 - val_loss: 3.7213e-04\n",
      "Epoch 758/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6474e-04 - val_loss: 3.7036e-04\n",
      "Epoch 759/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6450e-04 - val_loss: 3.7047e-04\n",
      "Epoch 760/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6435e-04 - val_loss: 3.7070e-04\n",
      "Epoch 761/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6451e-04 - val_loss: 3.7170e-04\n",
      "Epoch 762/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6424e-04 - val_loss: 3.7084e-04\n",
      "Epoch 763/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6405e-04 - val_loss: 3.7070e-04\n",
      "Epoch 764/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6433e-04 - val_loss: 3.7110e-04\n",
      "Epoch 765/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6418e-04 - val_loss: 3.7019e-04\n",
      "Epoch 766/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6423e-04 - val_loss: 3.7090e-04\n",
      "Epoch 767/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6436e-04 - val_loss: 3.7029e-04\n",
      "Epoch 768/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6389e-04 - val_loss: 3.6967e-04\n",
      "Epoch 769/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6408e-04 - val_loss: 3.7060e-04\n",
      "Epoch 770/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6392e-04 - val_loss: 3.7090e-04\n",
      "Epoch 771/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6372e-04 - val_loss: 3.7066e-04\n",
      "Epoch 772/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6355e-04 - val_loss: 3.7024e-04\n",
      "Epoch 773/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6367e-04 - val_loss: 3.7048e-04\n",
      "Epoch 774/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6354e-04 - val_loss: 3.7031e-04\n",
      "Epoch 775/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6370e-04 - val_loss: 3.7004e-04\n",
      "Epoch 776/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.6342e-04 - val_loss: 3.6997e-04\n",
      "Epoch 777/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6331e-04 - val_loss: 3.6939e-04\n",
      "Epoch 778/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6337e-04 - val_loss: 3.7000e-04\n",
      "Epoch 779/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6319e-04 - val_loss: 3.6974e-04\n",
      "Epoch 780/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6336e-04 - val_loss: 3.6997e-04\n",
      "Epoch 781/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6314e-04 - val_loss: 3.6990e-04\n",
      "Epoch 782/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6328e-04 - val_loss: 3.6930e-04\n",
      "Epoch 783/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6273e-04 - val_loss: 3.6943e-04\n",
      "Epoch 784/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6299e-04 - val_loss: 3.6981e-04\n",
      "Epoch 785/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6307e-04 - val_loss: 3.6927e-04\n",
      "Epoch 786/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6286e-04 - val_loss: 3.6866e-04\n",
      "Epoch 787/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6266e-04 - val_loss: 3.6895e-04\n",
      "Epoch 788/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6272e-04 - val_loss: 3.6919e-04\n",
      "Epoch 789/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6260e-04 - val_loss: 3.6872e-04\n",
      "Epoch 790/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6257e-04 - val_loss: 3.6874e-04\n",
      "Epoch 791/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6247e-04 - val_loss: 3.6889e-04\n",
      "Epoch 792/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6237e-04 - val_loss: 3.6879e-04\n",
      "Epoch 793/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6237e-04 - val_loss: 3.6992e-04\n",
      "Epoch 794/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6217e-04 - val_loss: 3.6861e-04\n",
      "Epoch 795/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6249e-04 - val_loss: 3.6920e-04\n",
      "Epoch 796/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6234e-04 - val_loss: 3.6868e-04\n",
      "Epoch 797/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6208e-04 - val_loss: 3.6921e-04\n",
      "Epoch 798/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6189e-04 - val_loss: 3.6845e-04\n",
      "Epoch 799/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6201e-04 - val_loss: 3.6849e-04\n",
      "Epoch 800/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6215e-04 - val_loss: 3.6954e-04\n",
      "Epoch 801/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6206e-04 - val_loss: 3.6802e-04\n",
      "Epoch 802/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6173e-04 - val_loss: 3.6845e-04\n",
      "Epoch 803/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6187e-04 - val_loss: 3.6848e-04\n",
      "Epoch 804/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6175e-04 - val_loss: 3.6844e-04\n",
      "Epoch 805/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6155e-04 - val_loss: 3.6786e-04\n",
      "Epoch 806/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6151e-04 - val_loss: 3.6843e-04\n",
      "Epoch 807/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6157e-04 - val_loss: 3.6790e-04\n",
      "Epoch 808/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6127e-04 - val_loss: 3.6815e-04\n",
      "Epoch 809/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6122e-04 - val_loss: 3.6860e-04\n",
      "Epoch 810/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6141e-04 - val_loss: 3.6975e-04\n",
      "Epoch 811/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6136e-04 - val_loss: 3.6780e-04\n",
      "Epoch 812/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6108e-04 - val_loss: 3.6758e-04\n",
      "Epoch 813/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6119e-04 - val_loss: 3.6752e-04\n",
      "Epoch 814/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6105e-04 - val_loss: 3.6776e-04\n",
      "Epoch 815/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6124e-04 - val_loss: 3.6831e-04\n",
      "Epoch 816/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6079e-04 - val_loss: 3.6854e-04\n",
      "Epoch 817/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6101e-04 - val_loss: 3.6801e-04\n",
      "Epoch 818/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6095e-04 - val_loss: 3.6704e-04\n",
      "Epoch 819/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6076e-04 - val_loss: 3.6705e-04\n",
      "Epoch 820/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6077e-04 - val_loss: 3.6684e-04\n",
      "Epoch 821/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6088e-04 - val_loss: 3.6746e-04\n",
      "Epoch 822/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6078e-04 - val_loss: 3.6783e-04\n",
      "Epoch 823/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6068e-04 - val_loss: 3.6739e-04\n",
      "Epoch 824/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6043e-04 - val_loss: 3.6760e-04\n",
      "Epoch 825/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6053e-04 - val_loss: 3.6674e-04\n",
      "Epoch 826/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6022e-04 - val_loss: 3.6686e-04\n",
      "Epoch 827/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6025e-04 - val_loss: 3.6678e-04\n",
      "Epoch 828/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6006e-04 - val_loss: 3.6707e-04\n",
      "Epoch 829/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6018e-04 - val_loss: 3.6758e-04\n",
      "Epoch 830/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6023e-04 - val_loss: 3.6721e-04\n",
      "Epoch 831/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5998e-04 - val_loss: 3.6650e-04\n",
      "Epoch 832/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5992e-04 - val_loss: 3.6596e-04\n",
      "Epoch 833/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5995e-04 - val_loss: 3.6720e-04\n",
      "Epoch 834/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6098e-04 - val_loss: 3.6656e-04\n",
      "Epoch 835/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.6011e-04 - val_loss: 3.6828e-04\n",
      "Epoch 836/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5981e-04 - val_loss: 3.6667e-04\n",
      "Epoch 837/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5980e-04 - val_loss: 3.6659e-04\n",
      "Epoch 838/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5980e-04 - val_loss: 3.6914e-04\n",
      "Epoch 839/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5993e-04 - val_loss: 3.6649e-04\n",
      "Epoch 840/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5967e-04 - val_loss: 3.6614e-04\n",
      "Epoch 841/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5941e-04 - val_loss: 3.6652e-04\n",
      "Epoch 842/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5942e-04 - val_loss: 3.6596e-04\n",
      "Epoch 843/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5947e-04 - val_loss: 3.6646e-04\n",
      "Epoch 844/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5934e-04 - val_loss: 3.6587e-04\n",
      "Epoch 845/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5930e-04 - val_loss: 3.6600e-04\n",
      "Epoch 846/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5930e-04 - val_loss: 3.6562e-04\n",
      "Epoch 847/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5911e-04 - val_loss: 3.6541e-04\n",
      "Epoch 848/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5892e-04 - val_loss: 3.6581e-04\n",
      "Epoch 849/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5912e-04 - val_loss: 3.6600e-04\n",
      "Epoch 850/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5917e-04 - val_loss: 3.6544e-04\n",
      "Epoch 851/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5900e-04 - val_loss: 3.6660e-04\n",
      "Epoch 852/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5871e-04 - val_loss: 3.6574e-04\n",
      "Epoch 853/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5867e-04 - val_loss: 3.6616e-04\n",
      "Epoch 854/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5895e-04 - val_loss: 3.6747e-04\n",
      "Epoch 855/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5893e-04 - val_loss: 3.6626e-04\n",
      "Epoch 856/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5844e-04 - val_loss: 3.6562e-04\n",
      "Epoch 857/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5846e-04 - val_loss: 3.6655e-04\n",
      "Epoch 858/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5846e-04 - val_loss: 3.6571e-04\n",
      "Epoch 859/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5858e-04 - val_loss: 3.6691e-04\n",
      "Epoch 860/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5862e-04 - val_loss: 3.6495e-04\n",
      "Epoch 861/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5825e-04 - val_loss: 3.6612e-04\n",
      "Epoch 862/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5814e-04 - val_loss: 3.6541e-04\n",
      "Epoch 863/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.5826e-04 - val_loss: 3.6463e-04\n",
      "Epoch 864/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5823e-04 - val_loss: 3.6514e-04\n",
      "Epoch 865/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5822e-04 - val_loss: 3.6583e-04\n",
      "Epoch 866/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5814e-04 - val_loss: 3.6524e-04\n",
      "Epoch 867/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.5811e-04 - val_loss: 3.6496e-04\n",
      "Epoch 868/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.5827e-04 - val_loss: 3.6497e-04\n",
      "Epoch 869/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5788e-04 - val_loss: 3.6537e-04\n",
      "Epoch 870/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5775e-04 - val_loss: 3.6447e-04\n",
      "Epoch 871/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5765e-04 - val_loss: 3.6486e-04\n",
      "Epoch 872/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5756e-04 - val_loss: 3.6397e-04\n",
      "Epoch 873/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5763e-04 - val_loss: 3.6443e-04\n",
      "Epoch 874/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5773e-04 - val_loss: 3.6563e-04\n",
      "Epoch 875/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5761e-04 - val_loss: 3.6491e-04\n",
      "Epoch 876/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5752e-04 - val_loss: 3.6417e-04\n",
      "Epoch 877/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5748e-04 - val_loss: 3.6421e-04\n",
      "Epoch 878/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5740e-04 - val_loss: 3.6466e-04\n",
      "Epoch 879/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5739e-04 - val_loss: 3.6503e-04\n",
      "Epoch 880/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5715e-04 - val_loss: 3.6388e-04\n",
      "Epoch 881/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.5747e-04 - val_loss: 3.6491e-04\n",
      "Epoch 882/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5742e-04 - val_loss: 3.6544e-04\n",
      "Epoch 883/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5749e-04 - val_loss: 3.6451e-04\n",
      "Epoch 884/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5706e-04 - val_loss: 3.6531e-04\n",
      "Epoch 885/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5715e-04 - val_loss: 3.6394e-04\n",
      "Epoch 886/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5693e-04 - val_loss: 3.6446e-04\n",
      "Epoch 887/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5690e-04 - val_loss: 3.6425e-04\n",
      "Epoch 888/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5688e-04 - val_loss: 3.6483e-04\n",
      "Epoch 889/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5708e-04 - val_loss: 3.6390e-04\n",
      "Epoch 890/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5689e-04 - val_loss: 3.6391e-04\n",
      "Epoch 891/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5658e-04 - val_loss: 3.6478e-04\n",
      "Epoch 892/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5659e-04 - val_loss: 3.6411e-04\n",
      "Epoch 893/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5655e-04 - val_loss: 3.6371e-04\n",
      "Epoch 894/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5651e-04 - val_loss: 3.6347e-04\n",
      "Epoch 895/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5632e-04 - val_loss: 3.6397e-04\n",
      "Epoch 896/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5640e-04 - val_loss: 3.6383e-04\n",
      "Epoch 897/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5633e-04 - val_loss: 3.6396e-04\n",
      "Epoch 898/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5653e-04 - val_loss: 3.6302e-04\n",
      "Epoch 899/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5614e-04 - val_loss: 3.6258e-04\n",
      "Epoch 900/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5618e-04 - val_loss: 3.6279e-04\n",
      "Epoch 901/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5624e-04 - val_loss: 3.6375e-04\n",
      "Epoch 902/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5603e-04 - val_loss: 3.6353e-04\n",
      "Epoch 903/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5588e-04 - val_loss: 3.6364e-04\n",
      "Epoch 904/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5587e-04 - val_loss: 3.6345e-04\n",
      "Epoch 905/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5604e-04 - val_loss: 3.6358e-04\n",
      "Epoch 906/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5616e-04 - val_loss: 3.6340e-04\n",
      "Epoch 907/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5587e-04 - val_loss: 3.6377e-04\n",
      "Epoch 908/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5592e-04 - val_loss: 3.6289e-04\n",
      "Epoch 909/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5566e-04 - val_loss: 3.6247e-04\n",
      "Epoch 910/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5579e-04 - val_loss: 3.6292e-04\n",
      "Epoch 911/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5558e-04 - val_loss: 3.6245e-04\n",
      "Epoch 912/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5548e-04 - val_loss: 3.6243e-04\n",
      "Epoch 913/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5564e-04 - val_loss: 3.6245e-04\n",
      "Epoch 914/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5540e-04 - val_loss: 3.6257e-04\n",
      "Epoch 915/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5537e-04 - val_loss: 3.6249e-04\n",
      "Epoch 916/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5559e-04 - val_loss: 3.6277e-04\n",
      "Epoch 917/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5538e-04 - val_loss: 3.6245e-04\n",
      "Epoch 918/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5503e-04 - val_loss: 3.6177e-04\n",
      "Epoch 919/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5531e-04 - val_loss: 3.6193e-04\n",
      "Epoch 920/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5516e-04 - val_loss: 3.6216e-04\n",
      "Epoch 921/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5528e-04 - val_loss: 3.6180e-04\n",
      "Epoch 922/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5492e-04 - val_loss: 3.6187e-04\n",
      "Epoch 923/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5487e-04 - val_loss: 3.6218e-04\n",
      "Epoch 924/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5497e-04 - val_loss: 3.6232e-04\n",
      "Epoch 925/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5500e-04 - val_loss: 3.6218e-04\n",
      "Epoch 926/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5478e-04 - val_loss: 3.6174e-04\n",
      "Epoch 927/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5480e-04 - val_loss: 3.6260e-04\n",
      "Epoch 928/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5468e-04 - val_loss: 3.6207e-04\n",
      "Epoch 929/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5438e-04 - val_loss: 3.6189e-04\n",
      "Epoch 930/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5473e-04 - val_loss: 3.6170e-04\n",
      "Epoch 931/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5472e-04 - val_loss: 3.6161e-04\n",
      "Epoch 932/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5462e-04 - val_loss: 3.6185e-04\n",
      "Epoch 933/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5454e-04 - val_loss: 3.6156e-04\n",
      "Epoch 934/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5431e-04 - val_loss: 3.6269e-04\n",
      "Epoch 935/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5432e-04 - val_loss: 3.6176e-04\n",
      "Epoch 936/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5422e-04 - val_loss: 3.6124e-04\n",
      "Epoch 937/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5419e-04 - val_loss: 3.6204e-04\n",
      "Epoch 938/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5418e-04 - val_loss: 3.6169e-04\n",
      "Epoch 939/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5415e-04 - val_loss: 3.6134e-04\n",
      "Epoch 940/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5402e-04 - val_loss: 3.6117e-04\n",
      "Epoch 941/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5395e-04 - val_loss: 3.6109e-04\n",
      "Epoch 942/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5399e-04 - val_loss: 3.6121e-04\n",
      "Epoch 943/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5410e-04 - val_loss: 3.6084e-04\n",
      "Epoch 944/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5371e-04 - val_loss: 3.6102e-04\n",
      "Epoch 945/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5396e-04 - val_loss: 3.6133e-04\n",
      "Epoch 946/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5374e-04 - val_loss: 3.6104e-04\n",
      "Epoch 947/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5365e-04 - val_loss: 3.6097e-04\n",
      "Epoch 948/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5371e-04 - val_loss: 3.6179e-04\n",
      "Epoch 949/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5377e-04 - val_loss: 3.6135e-04\n",
      "Epoch 950/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5386e-04 - val_loss: 3.6099e-04\n",
      "Epoch 951/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5352e-04 - val_loss: 3.6062e-04\n",
      "Epoch 952/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5335e-04 - val_loss: 3.6081e-04\n",
      "Epoch 953/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5354e-04 - val_loss: 3.6120e-04\n",
      "Epoch 954/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5370e-04 - val_loss: 3.6053e-04\n",
      "Epoch 955/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5302e-04 - val_loss: 3.6051e-04\n",
      "Epoch 956/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5322e-04 - val_loss: 3.6105e-04\n",
      "Epoch 957/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.5363e-04 - val_loss: 3.6033e-04\n",
      "Epoch 958/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.5320e-04 - val_loss: 3.6103e-04\n",
      "Epoch 959/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5316e-04 - val_loss: 3.6047e-04\n",
      "Epoch 960/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5314e-04 - val_loss: 3.6048e-04\n",
      "Epoch 961/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5299e-04 - val_loss: 3.6035e-04\n",
      "Epoch 962/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5287e-04 - val_loss: 3.6045e-04\n",
      "Epoch 963/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5303e-04 - val_loss: 3.6060e-04\n",
      "Epoch 964/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5300e-04 - val_loss: 3.5995e-04\n",
      "Epoch 965/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5260e-04 - val_loss: 3.6059e-04\n",
      "Epoch 966/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5282e-04 - val_loss: 3.6078e-04\n",
      "Epoch 967/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.5289e-04 - val_loss: 3.6061e-04\n",
      "Epoch 968/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5268e-04 - val_loss: 3.5942e-04\n",
      "Epoch 969/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5266e-04 - val_loss: 3.6008e-04\n",
      "Epoch 970/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5258e-04 - val_loss: 3.6020e-04\n",
      "Epoch 971/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5234e-04 - val_loss: 3.6023e-04\n",
      "Epoch 972/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5243e-04 - val_loss: 3.5991e-04\n",
      "Epoch 973/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5254e-04 - val_loss: 3.6004e-04\n",
      "Epoch 974/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5234e-04 - val_loss: 3.5972e-04\n",
      "Epoch 975/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5227e-04 - val_loss: 3.5972e-04\n",
      "Epoch 976/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5216e-04 - val_loss: 3.6008e-04\n",
      "Epoch 977/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5242e-04 - val_loss: 3.5931e-04\n",
      "Epoch 978/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.5221e-04 - val_loss: 3.5965e-04\n",
      "Epoch 979/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5208e-04 - val_loss: 3.5911e-04\n",
      "Epoch 980/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5192e-04 - val_loss: 3.5972e-04\n",
      "Epoch 981/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5210e-04 - val_loss: 3.5921e-04\n",
      "Epoch 982/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5196e-04 - val_loss: 3.5936e-04\n",
      "Epoch 983/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5204e-04 - val_loss: 3.5891e-04\n",
      "Epoch 984/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5167e-04 - val_loss: 3.6010e-04\n",
      "Epoch 985/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5195e-04 - val_loss: 3.5903e-04\n",
      "Epoch 986/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5149e-04 - val_loss: 3.5940e-04\n",
      "Epoch 987/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5162e-04 - val_loss: 3.5933e-04\n",
      "Epoch 988/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5166e-04 - val_loss: 3.5951e-04\n",
      "Epoch 989/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5142e-04 - val_loss: 3.5851e-04\n",
      "Epoch 990/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5175e-04 - val_loss: 3.5897e-04\n",
      "Epoch 991/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5135e-04 - val_loss: 3.5917e-04\n",
      "Epoch 992/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5109e-04 - val_loss: 3.5857e-04\n",
      "Epoch 993/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5139e-04 - val_loss: 3.5966e-04\n",
      "Epoch 994/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5122e-04 - val_loss: 3.5893e-04\n",
      "Epoch 995/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5159e-04 - val_loss: 3.5947e-04\n",
      "Epoch 996/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5118e-04 - val_loss: 3.5859e-04\n",
      "Epoch 997/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5113e-04 - val_loss: 3.5930e-04\n",
      "Epoch 998/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5131e-04 - val_loss: 3.5831e-04\n",
      "Epoch 999/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5103e-04 - val_loss: 3.5956e-04\n",
      "Epoch 1000/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5085e-04 - val_loss: 3.5831e-04\n",
      "INFO:tensorflow:Assets written to: AET_CF_Trial_12/models/saved_model_12_LSP_AET_CF_0/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: AET_CF_Trial_12/models/saved_model_12_LSP_AET_CF_0/assets\n",
      "  2%|▋                                      | 1/59 [22:38<21:53:13, 1358.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5076e-04 - val_loss: 3.6087e-04\n",
      "Epoch 2/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5086e-04 - val_loss: 3.5820e-04\n",
      "Epoch 3/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5096e-04 - val_loss: 3.5852e-04\n",
      "Epoch 4/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5079e-04 - val_loss: 3.5826e-04\n",
      "Epoch 5/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5067e-04 - val_loss: 3.5850e-04\n",
      "Epoch 6/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5085e-04 - val_loss: 3.5808e-04\n",
      "Epoch 7/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5091e-04 - val_loss: 3.5856e-04\n",
      "Epoch 8/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5088e-04 - val_loss: 3.5787e-04\n",
      "Epoch 9/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5042e-04 - val_loss: 3.5904e-04\n",
      "Epoch 10/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5052e-04 - val_loss: 3.5880e-04\n",
      "Epoch 11/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5040e-04 - val_loss: 3.5842e-04\n",
      "Epoch 12/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5048e-04 - val_loss: 3.5812e-04\n",
      "Epoch 13/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5022e-04 - val_loss: 3.5775e-04\n",
      "Epoch 14/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5050e-04 - val_loss: 3.5868e-04\n",
      "Epoch 15/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5035e-04 - val_loss: 3.5765e-04\n",
      "Epoch 16/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5034e-04 - val_loss: 3.5853e-04\n",
      "Epoch 17/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5029e-04 - val_loss: 3.5804e-04\n",
      "Epoch 18/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5013e-04 - val_loss: 3.5909e-04\n",
      "Epoch 19/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5016e-04 - val_loss: 3.5818e-04\n",
      "Epoch 20/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5000e-04 - val_loss: 3.5757e-04\n",
      "Epoch 21/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4999e-04 - val_loss: 3.5843e-04\n",
      "Epoch 22/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4993e-04 - val_loss: 3.5802e-04\n",
      "Epoch 23/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4975e-04 - val_loss: 3.5800e-04\n",
      "Epoch 24/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.5024e-04 - val_loss: 3.5744e-04\n",
      "Epoch 25/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4973e-04 - val_loss: 3.5696e-04\n",
      "Epoch 26/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4970e-04 - val_loss: 3.5735e-04\n",
      "Epoch 27/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4955e-04 - val_loss: 3.5738e-04\n",
      "Epoch 28/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4951e-04 - val_loss: 3.5857e-04\n",
      "Epoch 29/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4949e-04 - val_loss: 3.5776e-04\n",
      "Epoch 30/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4940e-04 - val_loss: 3.5719e-04\n",
      "Epoch 31/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4953e-04 - val_loss: 3.5736e-04\n",
      "Epoch 32/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4942e-04 - val_loss: 3.5702e-04\n",
      "Epoch 33/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4935e-04 - val_loss: 3.5715e-04\n",
      "Epoch 34/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.4938e-04 - val_loss: 3.5898e-04\n",
      "Epoch 35/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4923e-04 - val_loss: 3.5701e-04\n",
      "Epoch 36/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4932e-04 - val_loss: 3.5771e-04\n",
      "Epoch 37/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4927e-04 - val_loss: 3.5741e-04\n",
      "Epoch 38/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4917e-04 - val_loss: 3.5640e-04\n",
      "Epoch 39/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4903e-04 - val_loss: 3.5718e-04\n",
      "Epoch 40/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4899e-04 - val_loss: 3.5638e-04\n",
      "Epoch 41/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4915e-04 - val_loss: 3.5682e-04\n",
      "Epoch 42/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4894e-04 - val_loss: 3.5666e-04\n",
      "Epoch 43/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4909e-04 - val_loss: 3.5652e-04\n",
      "Epoch 44/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4906e-04 - val_loss: 3.5655e-04\n",
      "Epoch 45/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4876e-04 - val_loss: 3.5688e-04\n",
      "Epoch 46/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4865e-04 - val_loss: 3.5648e-04\n",
      "Epoch 47/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4866e-04 - val_loss: 3.5743e-04\n",
      "Epoch 48/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4855e-04 - val_loss: 3.5802e-04\n",
      "Epoch 49/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4884e-04 - val_loss: 3.5586e-04\n",
      "Epoch 50/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4864e-04 - val_loss: 3.5575e-04\n",
      "Epoch 51/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4856e-04 - val_loss: 3.5607e-04\n",
      "Epoch 52/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4826e-04 - val_loss: 3.5658e-04\n",
      "Epoch 53/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4855e-04 - val_loss: 3.5565e-04\n",
      "Epoch 54/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4837e-04 - val_loss: 3.5633e-04\n",
      "Epoch 55/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4840e-04 - val_loss: 3.5582e-04\n",
      "Epoch 56/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4827e-04 - val_loss: 3.5717e-04\n",
      "Epoch 57/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4819e-04 - val_loss: 3.5578e-04\n",
      "Epoch 58/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4828e-04 - val_loss: 3.5550e-04\n",
      "Epoch 59/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4813e-04 - val_loss: 3.5585e-04\n",
      "Epoch 60/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4809e-04 - val_loss: 3.5582e-04\n",
      "Epoch 61/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4810e-04 - val_loss: 3.5592e-04\n",
      "Epoch 62/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4786e-04 - val_loss: 3.5554e-04\n",
      "Epoch 63/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.4782e-04 - val_loss: 3.5586e-04\n",
      "Epoch 64/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4791e-04 - val_loss: 3.5546e-04\n",
      "Epoch 65/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4795e-04 - val_loss: 3.5642e-04\n",
      "Epoch 66/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4770e-04 - val_loss: 3.5522e-04\n",
      "Epoch 67/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4760e-04 - val_loss: 3.5637e-04\n",
      "Epoch 68/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.4763e-04 - val_loss: 3.5584e-04\n",
      "Epoch 69/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4760e-04 - val_loss: 3.5545e-04\n",
      "Epoch 70/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4763e-04 - val_loss: 3.5541e-04\n",
      "Epoch 71/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4761e-04 - val_loss: 3.5507e-04\n",
      "Epoch 72/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4749e-04 - val_loss: 3.5494e-04\n",
      "Epoch 73/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4746e-04 - val_loss: 3.5492e-04\n",
      "Epoch 74/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4773e-04 - val_loss: 3.5513e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4736e-04 - val_loss: 3.5563e-04\n",
      "Epoch 76/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4748e-04 - val_loss: 3.5575e-04\n",
      "Epoch 77/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4734e-04 - val_loss: 3.5502e-04\n",
      "Epoch 78/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4724e-04 - val_loss: 3.5528e-04\n",
      "Epoch 79/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4716e-04 - val_loss: 3.5538e-04\n",
      "Epoch 80/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4730e-04 - val_loss: 3.5592e-04\n",
      "Epoch 81/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4708e-04 - val_loss: 3.5464e-04\n",
      "Epoch 82/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4702e-04 - val_loss: 3.5537e-04\n",
      "Epoch 83/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4688e-04 - val_loss: 3.5421e-04\n",
      "Epoch 84/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4713e-04 - val_loss: 3.5491e-04\n",
      "Epoch 85/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4703e-04 - val_loss: 3.5493e-04\n",
      "Epoch 86/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4682e-04 - val_loss: 3.5444e-04\n",
      "Epoch 87/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4671e-04 - val_loss: 3.5498e-04\n",
      "Epoch 88/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4688e-04 - val_loss: 3.5515e-04\n",
      "Epoch 89/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4679e-04 - val_loss: 3.5589e-04\n",
      "Epoch 90/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4683e-04 - val_loss: 3.5466e-04\n",
      "Epoch 91/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4672e-04 - val_loss: 3.5479e-04\n",
      "Epoch 92/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4671e-04 - val_loss: 3.5504e-04\n",
      "Epoch 93/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4676e-04 - val_loss: 3.5449e-04\n",
      "Epoch 94/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4669e-04 - val_loss: 3.5458e-04\n",
      "Epoch 95/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4643e-04 - val_loss: 3.5423e-04\n",
      "Epoch 96/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4650e-04 - val_loss: 3.5424e-04\n",
      "Epoch 97/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4633e-04 - val_loss: 3.5438e-04\n",
      "Epoch 98/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4618e-04 - val_loss: 3.5433e-04\n",
      "Epoch 99/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4659e-04 - val_loss: 3.5445e-04\n",
      "Epoch 100/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4633e-04 - val_loss: 3.5526e-04\n",
      "Epoch 101/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4605e-04 - val_loss: 3.5362e-04\n",
      "Epoch 102/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4618e-04 - val_loss: 3.5568e-04\n",
      "Epoch 103/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4600e-04 - val_loss: 3.5494e-04\n",
      "Epoch 104/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4615e-04 - val_loss: 3.5420e-04\n",
      "Epoch 105/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4607e-04 - val_loss: 3.5398e-04\n",
      "Epoch 106/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4589e-04 - val_loss: 3.5373e-04\n",
      "Epoch 107/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4605e-04 - val_loss: 3.5429e-04\n",
      "Epoch 108/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4611e-04 - val_loss: 3.5373e-04\n",
      "Epoch 109/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4577e-04 - val_loss: 3.5341e-04\n",
      "Epoch 110/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4589e-04 - val_loss: 3.5380e-04\n",
      "Epoch 111/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4588e-04 - val_loss: 3.5374e-04\n",
      "Epoch 112/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4614e-04 - val_loss: 3.5381e-04\n",
      "Epoch 113/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4576e-04 - val_loss: 3.5353e-04\n",
      "Epoch 114/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4570e-04 - val_loss: 3.5332e-04\n",
      "Epoch 115/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4566e-04 - val_loss: 3.5432e-04\n",
      "Epoch 116/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4546e-04 - val_loss: 3.5415e-04\n",
      "Epoch 117/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4554e-04 - val_loss: 3.5381e-04\n",
      "Epoch 118/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4556e-04 - val_loss: 3.5344e-04\n",
      "Epoch 119/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4555e-04 - val_loss: 3.5470e-04\n",
      "Epoch 120/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4527e-04 - val_loss: 3.5395e-04\n",
      "Epoch 121/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4534e-04 - val_loss: 3.5365e-04\n",
      "Epoch 122/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4520e-04 - val_loss: 3.5355e-04\n",
      "Epoch 123/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4523e-04 - val_loss: 3.5296e-04\n",
      "Epoch 124/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4498e-04 - val_loss: 3.5396e-04\n",
      "Epoch 125/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4524e-04 - val_loss: 3.5442e-04\n",
      "Epoch 126/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4507e-04 - val_loss: 3.5355e-04\n",
      "Epoch 127/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4521e-04 - val_loss: 3.5306e-04\n",
      "Epoch 128/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4495e-04 - val_loss: 3.5322e-04\n",
      "Epoch 129/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4493e-04 - val_loss: 3.5309e-04\n",
      "Epoch 130/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4502e-04 - val_loss: 3.5290e-04\n",
      "Epoch 131/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4474e-04 - val_loss: 3.5266e-04\n",
      "Epoch 132/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4481e-04 - val_loss: 3.5247e-04\n",
      "Epoch 133/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4475e-04 - val_loss: 3.5220e-04\n",
      "Epoch 134/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4467e-04 - val_loss: 3.5389e-04\n",
      "Epoch 135/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4474e-04 - val_loss: 3.5313e-04\n",
      "Epoch 136/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4485e-04 - val_loss: 3.5271e-04\n",
      "Epoch 137/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4480e-04 - val_loss: 3.5256e-04\n",
      "Epoch 138/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4474e-04 - val_loss: 3.5267e-04\n",
      "Epoch 139/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4448e-04 - val_loss: 3.5294e-04\n",
      "Epoch 140/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4456e-04 - val_loss: 3.5245e-04\n",
      "Epoch 141/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4426e-04 - val_loss: 3.5220e-04\n",
      "Epoch 142/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4437e-04 - val_loss: 3.5213e-04\n",
      "Epoch 143/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4434e-04 - val_loss: 3.5251e-04\n",
      "Epoch 144/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4426e-04 - val_loss: 3.5237e-04\n",
      "Epoch 145/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4451e-04 - val_loss: 3.5280e-04\n",
      "Epoch 146/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4454e-04 - val_loss: 3.5202e-04\n",
      "Epoch 147/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4427e-04 - val_loss: 3.5343e-04\n",
      "Epoch 148/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4403e-04 - val_loss: 3.5265e-04\n",
      "Epoch 149/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4408e-04 - val_loss: 3.5196e-04\n",
      "Epoch 150/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4415e-04 - val_loss: 3.5258e-04\n",
      "Epoch 151/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4407e-04 - val_loss: 3.5271e-04\n",
      "Epoch 152/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4413e-04 - val_loss: 3.5196e-04\n",
      "Epoch 153/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4412e-04 - val_loss: 3.5205e-04\n",
      "Epoch 154/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4407e-04 - val_loss: 3.5221e-04\n",
      "Epoch 155/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4388e-04 - val_loss: 3.5218e-04\n",
      "Epoch 156/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4378e-04 - val_loss: 3.5183e-04\n",
      "Epoch 157/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4368e-04 - val_loss: 3.5215e-04\n",
      "Epoch 158/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4370e-04 - val_loss: 3.5193e-04\n",
      "Epoch 159/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4375e-04 - val_loss: 3.5233e-04\n",
      "Epoch 160/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4367e-04 - val_loss: 3.5220e-04\n",
      "Epoch 161/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4358e-04 - val_loss: 3.5187e-04\n",
      "Epoch 162/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4369e-04 - val_loss: 3.5260e-04\n",
      "Epoch 163/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4366e-04 - val_loss: 3.5419e-04\n",
      "Epoch 164/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4337e-04 - val_loss: 3.5175e-04\n",
      "Epoch 165/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4355e-04 - val_loss: 3.5153e-04\n",
      "Epoch 166/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4360e-04 - val_loss: 3.5145e-04\n",
      "Epoch 167/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4342e-04 - val_loss: 3.5113e-04\n",
      "Epoch 168/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4317e-04 - val_loss: 3.5267e-04\n",
      "Epoch 169/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4336e-04 - val_loss: 3.5176e-04\n",
      "Epoch 170/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4346e-04 - val_loss: 3.5143e-04\n",
      "Epoch 171/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4316e-04 - val_loss: 3.5185e-04\n",
      "Epoch 172/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4302e-04 - val_loss: 3.5176e-04\n",
      "Epoch 173/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4305e-04 - val_loss: 3.5126e-04\n",
      "Epoch 174/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4325e-04 - val_loss: 3.5181e-04\n",
      "Epoch 175/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4306e-04 - val_loss: 3.5232e-04\n",
      "Epoch 176/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4319e-04 - val_loss: 3.5142e-04\n",
      "Epoch 177/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4295e-04 - val_loss: 3.5151e-04\n",
      "Epoch 178/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4296e-04 - val_loss: 3.5187e-04\n",
      "Epoch 179/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4286e-04 - val_loss: 3.5228e-04\n",
      "Epoch 180/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4272e-04 - val_loss: 3.5182e-04\n",
      "Epoch 181/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4285e-04 - val_loss: 3.5098e-04\n",
      "Epoch 182/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4257e-04 - val_loss: 3.5057e-04\n",
      "Epoch 183/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4298e-04 - val_loss: 3.5065e-04\n",
      "Epoch 184/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4259e-04 - val_loss: 3.5077e-04\n",
      "Epoch 185/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4270e-04 - val_loss: 3.5159e-04\n",
      "Epoch 186/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4250e-04 - val_loss: 3.5021e-04\n",
      "Epoch 187/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4235e-04 - val_loss: 3.5056e-04\n",
      "Epoch 188/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4264e-04 - val_loss: 3.5081e-04\n",
      "Epoch 189/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.4279e-04 - val_loss: 3.5065e-04\n",
      "Epoch 190/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4247e-04 - val_loss: 3.5018e-04\n",
      "Epoch 191/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4280e-04 - val_loss: 3.5148e-04\n",
      "Epoch 192/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4241e-04 - val_loss: 3.5159e-04\n",
      "Epoch 193/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4228e-04 - val_loss: 3.5070e-04\n",
      "Epoch 194/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4228e-04 - val_loss: 3.5147e-04\n",
      "Epoch 195/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4224e-04 - val_loss: 3.5059e-04\n",
      "Epoch 196/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4213e-04 - val_loss: 3.5119e-04\n",
      "Epoch 197/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4225e-04 - val_loss: 3.5032e-04\n",
      "Epoch 198/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4210e-04 - val_loss: 3.5046e-04\n",
      "Epoch 199/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4214e-04 - val_loss: 3.4988e-04\n",
      "Epoch 200/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4240e-04 - val_loss: 3.5090e-04\n",
      "Epoch 201/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4188e-04 - val_loss: 3.5058e-04\n",
      "Epoch 202/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4202e-04 - val_loss: 3.4938e-04\n",
      "Epoch 203/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4206e-04 - val_loss: 3.5050e-04\n",
      "Epoch 204/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4187e-04 - val_loss: 3.4997e-04\n",
      "Epoch 205/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4207e-04 - val_loss: 3.5095e-04\n",
      "Epoch 206/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4183e-04 - val_loss: 3.5020e-04\n",
      "Epoch 207/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4202e-04 - val_loss: 3.4989e-04\n",
      "Epoch 208/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4167e-04 - val_loss: 3.4972e-04\n",
      "Epoch 209/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4144e-04 - val_loss: 3.5172e-04\n",
      "Epoch 210/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4176e-04 - val_loss: 3.5049e-04\n",
      "Epoch 211/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4158e-04 - val_loss: 3.5040e-04\n",
      "Epoch 212/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4138e-04 - val_loss: 3.5065e-04\n",
      "Epoch 213/1000\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 3.4166e-04 - val_loss: 3.5035e-04\n",
      "Epoch 214/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4153e-04 - val_loss: 3.5012e-04\n",
      "Epoch 215/1000\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 3.4141e-04 - val_loss: 3.5029e-04\n",
      "Epoch 216/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4128e-04 - val_loss: 3.4993e-04\n",
      "Epoch 217/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4143e-04 - val_loss: 3.5099e-04\n",
      "Epoch 218/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4140e-04 - val_loss: 3.4992e-04\n",
      "Epoch 219/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4137e-04 - val_loss: 3.5086e-04\n",
      "Epoch 220/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4115e-04 - val_loss: 3.4925e-04\n",
      "Epoch 221/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4115e-04 - val_loss: 3.4997e-04\n",
      "Epoch 222/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4099e-04 - val_loss: 3.4943e-04\n",
      "Epoch 223/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4115e-04 - val_loss: 3.4930e-04\n",
      "Epoch 224/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4097e-04 - val_loss: 3.4962e-04\n",
      "Epoch 225/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4104e-04 - val_loss: 3.4941e-04\n",
      "Epoch 226/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4146e-04 - val_loss: 3.5224e-04\n",
      "Epoch 227/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4118e-04 - val_loss: 3.4941e-04\n",
      "Epoch 228/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4102e-04 - val_loss: 3.4891e-04\n",
      "Epoch 229/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4084e-04 - val_loss: 3.5039e-04\n",
      "Epoch 230/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4082e-04 - val_loss: 3.5013e-04\n",
      "Epoch 231/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4072e-04 - val_loss: 3.4915e-04\n",
      "Epoch 232/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4066e-04 - val_loss: 3.4907e-04\n",
      "Epoch 233/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4056e-04 - val_loss: 3.4915e-04\n",
      "Epoch 234/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4078e-04 - val_loss: 3.4950e-04\n",
      "Epoch 235/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4059e-04 - val_loss: 3.4884e-04\n",
      "Epoch 236/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4071e-04 - val_loss: 3.4854e-04\n",
      "Epoch 237/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4059e-04 - val_loss: 3.4883e-04\n",
      "Epoch 238/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4057e-04 - val_loss: 3.4939e-04\n",
      "Epoch 239/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4065e-04 - val_loss: 3.4938e-04\n",
      "Epoch 240/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4057e-04 - val_loss: 3.4863e-04\n",
      "Epoch 241/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4034e-04 - val_loss: 3.4928e-04\n",
      "Epoch 242/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4033e-04 - val_loss: 3.4850e-04\n",
      "Epoch 243/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4022e-04 - val_loss: 3.4828e-04\n",
      "Epoch 244/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4072e-04 - val_loss: 3.4990e-04\n",
      "Epoch 245/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4025e-04 - val_loss: 3.4819e-04\n",
      "Epoch 246/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4043e-04 - val_loss: 3.4850e-04\n",
      "Epoch 247/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4022e-04 - val_loss: 3.4873e-04\n",
      "Epoch 248/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4013e-04 - val_loss: 3.4883e-04\n",
      "Epoch 249/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4024e-04 - val_loss: 3.4881e-04\n",
      "Epoch 250/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4023e-04 - val_loss: 3.4850e-04\n",
      "Epoch 251/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4000e-04 - val_loss: 3.4920e-04\n",
      "Epoch 252/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4000e-04 - val_loss: 3.4880e-04\n",
      "Epoch 253/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3998e-04 - val_loss: 3.4891e-04\n",
      "Epoch 254/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4030e-04 - val_loss: 3.4824e-04\n",
      "Epoch 255/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3979e-04 - val_loss: 3.4813e-04\n",
      "Epoch 256/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3972e-04 - val_loss: 3.4818e-04\n",
      "Epoch 257/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3985e-04 - val_loss: 3.4890e-04\n",
      "Epoch 258/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3968e-04 - val_loss: 3.4851e-04\n",
      "Epoch 259/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3982e-04 - val_loss: 3.4820e-04\n",
      "Epoch 260/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3970e-04 - val_loss: 3.4795e-04\n",
      "Epoch 261/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4002e-04 - val_loss: 3.4831e-04\n",
      "Epoch 262/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3960e-04 - val_loss: 3.4873e-04\n",
      "Epoch 263/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3950e-04 - val_loss: 3.4787e-04\n",
      "Epoch 264/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3949e-04 - val_loss: 3.4908e-04\n",
      "Epoch 265/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3952e-04 - val_loss: 3.4801e-04\n",
      "Epoch 266/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3954e-04 - val_loss: 3.4815e-04\n",
      "Epoch 267/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3942e-04 - val_loss: 3.4882e-04\n",
      "Epoch 268/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3933e-04 - val_loss: 3.4770e-04\n",
      "Epoch 269/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3927e-04 - val_loss: 3.4747e-04\n",
      "Epoch 270/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3922e-04 - val_loss: 3.4737e-04\n",
      "Epoch 271/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3934e-04 - val_loss: 3.4805e-04\n",
      "Epoch 272/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3918e-04 - val_loss: 3.4846e-04\n",
      "Epoch 273/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3912e-04 - val_loss: 3.4829e-04\n",
      "Epoch 274/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3932e-04 - val_loss: 3.4730e-04\n",
      "Epoch 275/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3899e-04 - val_loss: 3.4743e-04\n",
      "Epoch 276/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3920e-04 - val_loss: 3.4714e-04\n",
      "Epoch 277/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3896e-04 - val_loss: 3.4772e-04\n",
      "Epoch 278/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3903e-04 - val_loss: 3.4758e-04\n",
      "Epoch 279/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3905e-04 - val_loss: 3.4746e-04\n",
      "Epoch 280/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3920e-04 - val_loss: 3.4808e-04\n",
      "Epoch 281/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3885e-04 - val_loss: 3.4808e-04\n",
      "Epoch 282/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.4036e-04 - val_loss: 3.4861e-04\n",
      "Epoch 283/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3951e-04 - val_loss: 3.4718e-04\n",
      "Epoch 284/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3889e-04 - val_loss: 3.4721e-04\n",
      "Epoch 285/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3885e-04 - val_loss: 3.4696e-04\n",
      "Epoch 286/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3875e-04 - val_loss: 3.4688e-04\n",
      "Epoch 287/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3872e-04 - val_loss: 3.4823e-04\n",
      "Epoch 288/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3868e-04 - val_loss: 3.4715e-04\n",
      "Epoch 289/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3826e-04 - val_loss: 3.4756e-04\n",
      "Epoch 290/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3850e-04 - val_loss: 3.4717e-04\n",
      "Epoch 291/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3841e-04 - val_loss: 3.4857e-04\n",
      "Epoch 292/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3867e-04 - val_loss: 3.4872e-04\n",
      "Epoch 293/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3853e-04 - val_loss: 3.4841e-04\n",
      "Epoch 294/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3843e-04 - val_loss: 3.4815e-04\n",
      "Epoch 295/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3867e-04 - val_loss: 3.4734e-04\n",
      "Epoch 296/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3835e-04 - val_loss: 3.4731e-04\n",
      "Epoch 297/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3834e-04 - val_loss: 3.4704e-04\n",
      "Epoch 298/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3811e-04 - val_loss: 3.4772e-04\n",
      "Epoch 299/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3825e-04 - val_loss: 3.4726e-04\n",
      "Epoch 300/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3816e-04 - val_loss: 3.4644e-04\n",
      "Epoch 301/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3796e-04 - val_loss: 3.4682e-04\n",
      "Epoch 302/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3832e-04 - val_loss: 3.4697e-04\n",
      "Epoch 303/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3816e-04 - val_loss: 3.4631e-04\n",
      "Epoch 304/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3814e-04 - val_loss: 3.4663e-04\n",
      "Epoch 305/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3792e-04 - val_loss: 3.4630e-04\n",
      "Epoch 306/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3812e-04 - val_loss: 3.4744e-04\n",
      "Epoch 307/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3809e-04 - val_loss: 3.4653e-04\n",
      "Epoch 308/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3779e-04 - val_loss: 3.4726e-04\n",
      "Epoch 309/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3787e-04 - val_loss: 3.4683e-04\n",
      "Epoch 310/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3780e-04 - val_loss: 3.4623e-04\n",
      "Epoch 311/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3783e-04 - val_loss: 3.4766e-04\n",
      "Epoch 312/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3775e-04 - val_loss: 3.4717e-04\n",
      "Epoch 313/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3758e-04 - val_loss: 3.4613e-04\n",
      "Epoch 314/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3761e-04 - val_loss: 3.4654e-04\n",
      "Epoch 315/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3759e-04 - val_loss: 3.4647e-04\n",
      "Epoch 316/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3740e-04 - val_loss: 3.4606e-04\n",
      "Epoch 317/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3752e-04 - val_loss: 3.4641e-04\n",
      "Epoch 318/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3756e-04 - val_loss: 3.4633e-04\n",
      "Epoch 319/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3729e-04 - val_loss: 3.4603e-04\n",
      "Epoch 320/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3749e-04 - val_loss: 3.4689e-04\n",
      "Epoch 321/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3737e-04 - val_loss: 3.4654e-04\n",
      "Epoch 322/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3725e-04 - val_loss: 3.4577e-04\n",
      "Epoch 323/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3722e-04 - val_loss: 3.4722e-04\n",
      "Epoch 324/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3719e-04 - val_loss: 3.4635e-04\n",
      "Epoch 325/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3764e-04 - val_loss: 3.4597e-04\n",
      "Epoch 326/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3716e-04 - val_loss: 3.4643e-04\n",
      "Epoch 327/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3704e-04 - val_loss: 3.4604e-04\n",
      "Epoch 328/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3716e-04 - val_loss: 3.4700e-04\n",
      "Epoch 329/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3718e-04 - val_loss: 3.4588e-04\n",
      "Epoch 330/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3719e-04 - val_loss: 3.4596e-04\n",
      "Epoch 331/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3707e-04 - val_loss: 3.4576e-04\n",
      "Epoch 332/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3713e-04 - val_loss: 3.4901e-04\n",
      "Epoch 333/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3686e-04 - val_loss: 3.4678e-04\n",
      "Epoch 334/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3696e-04 - val_loss: 3.4635e-04\n",
      "Epoch 335/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3679e-04 - val_loss: 3.4580e-04\n",
      "Epoch 336/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3661e-04 - val_loss: 3.4717e-04\n",
      "Epoch 337/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3696e-04 - val_loss: 3.4595e-04\n",
      "Epoch 338/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3676e-04 - val_loss: 3.4574e-04\n",
      "Epoch 339/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3681e-04 - val_loss: 3.4584e-04\n",
      "Epoch 340/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3658e-04 - val_loss: 3.4545e-04\n",
      "Epoch 341/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3640e-04 - val_loss: 3.4561e-04\n",
      "Epoch 342/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3673e-04 - val_loss: 3.4560e-04\n",
      "Epoch 343/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3638e-04 - val_loss: 3.4557e-04\n",
      "Epoch 344/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3652e-04 - val_loss: 3.4689e-04\n",
      "Epoch 345/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3661e-04 - val_loss: 3.4598e-04\n",
      "Epoch 346/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3643e-04 - val_loss: 3.4555e-04\n",
      "Epoch 347/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3619e-04 - val_loss: 3.4561e-04\n",
      "Epoch 348/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3641e-04 - val_loss: 3.4504e-04\n",
      "Epoch 349/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3624e-04 - val_loss: 3.4598e-04\n",
      "Epoch 350/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3693e-04 - val_loss: 3.4713e-04\n",
      "Epoch 351/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3638e-04 - val_loss: 3.4506e-04\n",
      "Epoch 352/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3626e-04 - val_loss: 3.4526e-04\n",
      "Epoch 353/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3630e-04 - val_loss: 3.4554e-04\n",
      "Epoch 354/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3602e-04 - val_loss: 3.4523e-04\n",
      "Epoch 355/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3614e-04 - val_loss: 3.4424e-04\n",
      "Epoch 356/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3609e-04 - val_loss: 3.4501e-04\n",
      "Epoch 357/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3615e-04 - val_loss: 3.4525e-04\n",
      "Epoch 358/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3614e-04 - val_loss: 3.4442e-04\n",
      "Epoch 359/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3583e-04 - val_loss: 3.4510e-04\n",
      "Epoch 360/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3596e-04 - val_loss: 3.4484e-04\n",
      "Epoch 361/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3585e-04 - val_loss: 3.4452e-04\n",
      "Epoch 362/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3593e-04 - val_loss: 3.4502e-04\n",
      "Epoch 363/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3614e-04 - val_loss: 3.4510e-04\n",
      "Epoch 364/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3625e-04 - val_loss: 3.4549e-04\n",
      "Epoch 365/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3572e-04 - val_loss: 3.4496e-04\n",
      "Epoch 366/1000\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 3.3554e-04 - val_loss: 3.4454e-04\n",
      "Epoch 367/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169/250 [===================>..........] - ETA: 0s - loss: 3.3535e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "for counts in tqdm(range(59)):\n",
    "    output_path = os.path.join(folder_name, name + f\"Predictions{counts}.txt\")\n",
    "    with open(output_path, \"w\") as file:\n",
    "        history = autoencoder.fit(X_train_f, X_train_f, batch_size=256, epochs=1000,\n",
    "                                  validation_data=(X_valid_f, X_valid_f), verbose=1)\n",
    "\n",
    "        training_history = pd.DataFrame(history.history)\n",
    "        plt.plot(training_history)\n",
    "        file_name_0 = os.path.join(folder_name, name + \"Training_History\" + str(counts))\n",
    "        training_history.to_pickle(file_name_0)\n",
    "        file_name_1 = os.path.join(folder_name, name + \"Training_History\" + str(counts) + \".png\")\n",
    "        plt.savefig(file_name_1, dpi=300)\n",
    "        plt.clf()\n",
    "\n",
    "        dr_model = tf.keras.models.Model(inputs=autoencoder.get_layer('ae_input').input,\n",
    "                                         outputs=autoencoder.get_layer('ae_latent').output)\n",
    "        dr_model.summary(print_fn=lambda x: file.write(x + '\\n'))\n",
    "\n",
    "        batch_size = 32\n",
    "        x = []\n",
    "        y = []\n",
    "        z = []\n",
    "        indices = []\n",
    "\n",
    "        for batch_start in range(0, len(X_valid_f), batch_size):\n",
    "            batch_end = min(batch_start + batch_size, len(X_valid_f))\n",
    "            X_batch = np.array(X_valid_f.iloc[batch_start:batch_end])\n",
    "            y_batch = y_valid_f.iloc[batch_start:batch_end]\n",
    "\n",
    "            op_batch = dr_model.predict(X_batch, verbose=0)\n",
    "\n",
    "            for i, op in enumerate(op_batch):\n",
    "                z.append(y_batch.iloc[i]['class'])  # Access class label correctly\n",
    "                x.append(op[0])\n",
    "                y.append(op[1])\n",
    "                indices.append(y_batch.iloc[i].name)  # Using .name to get the index of the row\n",
    "                file.write(f\"Prediction {batch_start + i}: {op}\\n\")\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        df['x'] = x\n",
    "        df['y'] = y\n",
    "        df['z'] = [\"trajectory-\" + str(k) for k in z]\n",
    "        df['index'] = indices\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        fig = sns.scatterplot(x='x', y='y', hue='z', data=df, s=10)\n",
    "        file_name_2 = os.path.join(folder_name, name + 'Predictions' + str(counts) + \".png\")\n",
    "        fig.figure.savefig(file_name_2, dpi=300)\n",
    "        plt.clf()\n",
    "\n",
    "        file_name_3 = os.path.join(folder_name, name + 'Predictions' + str(counts))\n",
    "        df.to_pickle(file_name_3)\n",
    "\n",
    "        model_folder = os.path.join(folder_name, 'models')\n",
    "        if not os.path.exists(model_folder):\n",
    "            os.makedirs(model_folder)\n",
    "        file_name = os.path.join(model_folder, 'saved_model_' + name + str(counts))\n",
    "        autoencoder.save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d093fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Save index list\n",
    "shuffled_index_train = X_train_f.index.to_list()\n",
    "shuffled_index_valid = X_valid_f.index.to_list()\n",
    "\n",
    "# Save shuffled indices of the training data\n",
    "with open(f\"{folder_name}/shufftrain.npy\", \"wb\") as file:\n",
    "    np.save(file, shuffled_index_train)\n",
    "\n",
    "# Save shuffled indices of the validation data\n",
    "with open(f\"{folder_name}/shuffval.npy\", \"wb\") as file:\n",
    "    np.save(file, shuffled_index_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d778b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
